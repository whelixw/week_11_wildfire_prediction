{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11b7YAeVNrSf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmczLgfgNsgj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    roc_auc_score,\n",
        "    r2_score,\n",
        ")\n",
        "\n",
        "DATA_DIR = Path(\"data\")  # change to your local path\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kb3Y1NI4N5eF",
        "outputId": "2b85d8e0-aeb8-452d-c2c6-c1f795e5a1f4"
      },
      "outputs": [],
      "source": [
        "# ---- 1.1 Load wildfires ----\n",
        "fires_path = DATA_DIR / \"wildfires_fires.parquet\"  # adjust\n",
        "fires = pd.read_parquet(\n",
        "    fires_path,\n",
        "    columns=[\"FIRE_SIZE\", \"DISCOVERY_DATE\", \"STATE\", \"STAT_CAUSE_CODE\"],\n",
        ")\n",
        "\n",
        "# Convert DISCOVERY_DATE to datetime.\n",
        "# If it's already datetime, this will do nothing.\n",
        "# The error indicates that DISCOVERY_DATE is already datetime.date objects,\n",
        "# but not np.datetime64, so `unit='D', origin='julian'` is incompatible.\n",
        "# We can simply convert them to np.datetime64 directly.\n",
        "if not np.issubdtype(fires[\"DISCOVERY_DATE\"].dtype, np.datetime64):\n",
        "    fires[\"DISCOVERY_DATE\"] = pd.to_datetime(fires[\"DISCOVERY_DATE\"])\n",
        "\n",
        "fires = fires.rename(columns={\"DISCOVERY_DATE\": \"date\"})\n",
        "\n",
        "# Filter date range 1992–2015 (dataset range)\n",
        "fires = fires[(fires[\"date\"].dt.year >= 1992) & (fires[\"date\"].dt.year <= 2015)]\n",
        "\n",
        "fires.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "68N3QRYTOkxl",
        "outputId": "3c550150-3256-4209-e7ca-71f8cbdef0cd"
      },
      "outputs": [],
      "source": [
        "# ---- 1.2 Aggregate to state–day ----\n",
        "\n",
        "# Basic daily targets\n",
        "daily_agg = (\n",
        "    fires.groupby([\"STATE\", \"date\"])\n",
        "    .agg(\n",
        "        n_fires=(\"FIRE_SIZE\", \"size\"),\n",
        "        area_burned=(\"FIRE_SIZE\", \"sum\"),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Cause counts per state–day (wide format)\n",
        "cause_counts = (\n",
        "    fires.pivot_table(\n",
        "        index=[\"STATE\", \"date\"],\n",
        "        columns=\"STAT_CAUSE_CODE\",\n",
        "        values=\"FIRE_SIZE\",  # any column, we just count\n",
        "        aggfunc=\"count\",\n",
        "        fill_value=0,\n",
        "    )\n",
        "    .rename_axis(columns=\"cause_code\")\n",
        ")\n",
        "\n",
        "# Make nicer column names: cause_1, cause_2, ...\n",
        "cause_counts.columns = [f\"cause_{int(c)}\" for c in cause_counts.columns]\n",
        "\n",
        "cause_counts = cause_counts.reset_index()\n",
        "\n",
        "# Merge counts into daily_agg\n",
        "fires_daily = daily_agg.merge(\n",
        "    cause_counts, on=[\"STATE\", \"date\"], how=\"left\"\n",
        ").fillna(0)\n",
        "\n",
        "fires_daily[\"any_fire\"] = (fires_daily[\"n_fires\"] > 0).astype(int)\n",
        "\n",
        "fires_daily.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fx3pMs4GOqnU",
        "outputId": "64a1eb64-98d4-46c0-d7f0-2de80b74c8a3"
      },
      "outputs": [],
      "source": [
        "# ---- 2.1 Load weather ----\n",
        "weather_path = DATA_DIR / \"us_daily_weather_1992_2015.parquet\"  # adjust\n",
        "weather = pd.read_parquet(weather_path)\n",
        "\n",
        "# Load cities data to get state codes\n",
        "cities_path = DATA_DIR / \"cities.csv\"\n",
        "cities_df = pd.read_csv(cities_path)\n",
        "\n",
        "# Adapt these column names to your file\n",
        "# Here we assume:\n",
        "#   'date' (datetime), 'state' for 2-letter US state (corrected from state_code)\n",
        "#   'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'pres'\n",
        "weather[\"date\"] = pd.to_datetime(weather[\"date\"])\n",
        "\n",
        "# Restrict to 1992–2015 and US\n",
        "weather = weather[(weather[\"date\"].dt.year >= 1992) & (weather[\"date\"].dt.year <= 2015)]\n",
        "# If you already filtered to US, skip this; otherwise:\n",
        "# weather = weather[weather[\"country_code\"] == \"US\"]\n",
        "\n",
        "# Merge weather with cities to get state information\n",
        "weather = weather.merge(\n",
        "    cities_df[[\"city_name\", \"state\"]].drop_duplicates(), on=\"city_name\", how=\"left\"\n",
        ")\n",
        "\n",
        "# Rename the 'state' column from cities_df to 'STATE' to match fires_daily\n",
        "weather = weather.rename(columns={\"state\": \"STATE\",\n",
        "                                  \"avg_temp_c\" : \"tavg\",\n",
        "                                  \"min_temp_c\" : \"tmin\",\n",
        "                                  \"max_temp_c\" : \"tmax\",\n",
        "                                  \"precipitation_mm\" : \"prcp\",\n",
        "                                  \"snow_depth_mm\" : \"snow\",\n",
        "                                  \"avg_wind_speed_kmh\" : \"wspd\",\n",
        "                                  \"avg_sea_level_pres_hpa\" : \"pres\",})\n",
        "\n",
        "\n",
        "weather.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fvfLjQCtO5Je",
        "outputId": "ffc10472-2c8e-477d-df10-ba72bad0d400"
      },
      "outputs": [],
      "source": [
        "# ---- 2.2 Aggregate station data to state–day ----\n",
        "\n",
        "weather_daily = (\n",
        "    weather.groupby([\"STATE\", \"date\"])\n",
        "    .agg(\n",
        "        tavg=(\"tavg\", \"mean\"),\n",
        "        tmin=(\"tmin\", \"mean\"),\n",
        "        tmax=(\"tmax\", \"mean\"),\n",
        "        prcp=(\"prcp\", \"sum\"),\n",
        "        snow=(\"snow\", \"sum\"),\n",
        "        wspd=(\"wspd\", \"mean\"),\n",
        "        pres=(\"pres\", \"mean\"),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "weather_daily.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "3qIZ8DmPRM2h",
        "outputId": "2ce56e96-26bd-4a32-e8c8-3e7699d81edd"
      },
      "outputs": [],
      "source": [
        "# ---- 2.3 Add calendar features and lagged weather features ----\n",
        "\n",
        "def add_lagged_features(\n",
        "    df,\n",
        "    group_col,\n",
        "    date_col,\n",
        "    base_cols,\n",
        "    windows=(3, 7, 30),\n",
        "    shift=1,\n",
        "):\n",
        "    \"\"\"\n",
        "    For each base_col, compute rolling means or sums (depending on column name)\n",
        "    over given windows, then shift by `shift` days to avoid using same-day info.\n",
        "    \"\"\"\n",
        "    df = df.sort_values([group_col, date_col]).copy()\n",
        "\n",
        "    for col in base_cols:\n",
        "        for w in windows:\n",
        "            roll = (\n",
        "                df.groupby(group_col)[col]\n",
        "                .transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
        "            )\n",
        "            df[f\"{col}_mean_{w}\"] = roll.shift(shift)\n",
        "\n",
        "    # Example of cumulative precipitation (sums instead of means)\n",
        "    for w in [7, 30]:\n",
        "        roll = (\n",
        "            df.groupby(group_col)[\"prcp\"]\n",
        "            .transform(lambda x: x.rolling(w, min_periods=1).sum())\n",
        "        )\n",
        "        df[f\"prcp_sum_{w}\"] = roll.shift(shift)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "weather_feat = weather_daily.copy()\n",
        "\n",
        "# State name to abbreviation mapping to align with fires_daily\n",
        "state_name_to_abbrev = {\n",
        "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
        "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
        "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
        "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
        "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
        "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
        "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
        "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
        "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
        "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
        "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
        "    'Wisconsin': 'WI', 'Wyoming': 'WY',\n",
        "    'District of Columbia': 'DC'\n",
        "}\n",
        "\n",
        "\n",
        "# Apply the mapping to weather_feat['STATE']\n",
        "weather_feat['STATE'] = weather_feat['STATE'].map(state_name_to_abbrev)\n",
        "\n",
        "# Drop rows where state mapping resulted in NaN (if any states were not in our map)\n",
        "weather_feat.dropna(subset=['STATE'], inplace=True)\n",
        "\n",
        "weather_feat[\"year\"] = weather_feat[\"date\"].dt.year\n",
        "weather_feat[\"month\"] = weather_feat[\"date\"].dt.month\n",
        "weather_feat[\"doy\"] = weather_feat[\"date\"].dt.dayofyear\n",
        "\n",
        "base_cols = [\"tavg\", \"tmin\", \"tmax\", \"prcp\", \"wspd\", \"pres\"]\n",
        "weather_feat = add_lagged_features(\n",
        "    weather_feat,\n",
        "    group_col=\"STATE\",\n",
        "    date_col=\"date\",\n",
        "    base_cols=base_cols,\n",
        ")\n",
        "\n",
        "# Drop first few rows where lags are NaN\n",
        "weather_feat = weather_feat.dropna().reset_index(drop=True)\n",
        "\n",
        "weather_feat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "vlNE-HTcT3Om",
        "outputId": "0cde0de6-435e-4b56-99ba-8c299ed033c2"
      },
      "outputs": [],
      "source": [
        "# ---- 2.3 Add calendar features and lagged weather features ----\n",
        "\n",
        "def add_lagged_features(\n",
        "    df,\n",
        "    group_col,\n",
        "    date_col,\n",
        "    base_cols,\n",
        "    windows=(3, 7, 30),\n",
        "    shift=1,\n",
        "):\n",
        "    \"\"\"\n",
        "    For each base_col, compute rolling means or sums (depending on column name)\n",
        "    over given windows, then shift by `shift` days to avoid using same-day info.\n",
        "    \"\"\"\n",
        "    df = df.sort_values([group_col, date_col]).copy()\n",
        "\n",
        "    for col in base_cols:\n",
        "        for w in windows:\n",
        "            roll = (\n",
        "                df.groupby(group_col)[col]\n",
        "                .transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
        "            )\n",
        "            df[f\"{col}_mean_{w}\"] = roll.shift(shift)\n",
        "\n",
        "    # Example of cumulative precipitation (sums instead of means)\n",
        "    for w in [7, 30]:\n",
        "        roll = (\n",
        "            df.groupby(group_col)[\"prcp\"]\n",
        "            .transform(lambda x: x.rolling(w, min_periods=1).sum())\n",
        "        )\n",
        "        df[f\"prcp_sum_{w}\"] = roll.shift(shift)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "weather_feat = weather_daily.copy()\n",
        "\n",
        "# State name to abbreviation mapping to align with fires_daily\n",
        "state_name_to_abbrev = {\n",
        "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
        "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
        "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
        "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
        "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
        "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
        "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
        "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
        "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
        "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
        "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
        "    'Wisconsin': 'WI', 'Wyoming': 'WY',\n",
        "    'District of Columbia': 'DC'\n",
        "}\n",
        "\n",
        "# Apply the mapping to weather_feat['STATE']\n",
        "weather_feat['STATE'] = weather_feat['STATE'].map(state_name_to_abbrev)\n",
        "\n",
        "# Drop rows where state mapping resulted in NaN (if any states were not in our map)\n",
        "weather_feat.dropna(subset=['STATE'], inplace=True)\n",
        "\n",
        "# Impute any remaining NaNs in weather features before adding lagged features\n",
        "# This ensures lagged features don't become NaN due to missing base values.\n",
        "# Explicitly set 'STATE' as index for fill, then reset to column\n",
        "weather_feat = weather_feat.set_index('STATE')\n",
        "weather_feat = weather_feat.groupby(level=0).ffill().bfill()\n",
        "weather_feat = weather_feat.reset_index()\n",
        "\n",
        "weather_feat[\"year\"] = weather_feat[\"date\"].dt.year\n",
        "weather_feat[\"month\"] = weather_feat[\"date\"].dt.month\n",
        "weather_feat[\"doy\"] = weather_feat[\"date\"].dt.dayofyear\n",
        "\n",
        "base_cols = [\"tavg\", \"tmin\", \"tmax\", \"prcp\", \"wspd\", \"pres\"]\n",
        "weather_feat = add_lagged_features(\n",
        "    weather_feat,\n",
        "    group_col=\"STATE\",\n",
        "    date_col=\"date\",\n",
        "    base_cols=base_cols,\n",
        ")\n",
        "\n",
        "# Drop first few rows where lags are NaN (expected from the shift operation)\n",
        "weather_feat = weather_feat.dropna().reset_index(drop=True)\n",
        "\n",
        "weather_feat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "zmAkE0QyT5Tw",
        "outputId": "cbf29590-ac80-426c-c645-f99a2f7fe4e1"
      },
      "outputs": [],
      "source": [
        "data = weather_feat.merge(\n",
        "    fires_daily, on=[\"STATE\", \"date\"], how=\"left\"\n",
        ")\n",
        "\n",
        "# Replace NaNs in fire-related columns with 0\n",
        "fire_cols = [c for c in data.columns if c.startswith(\"cause_\")] + [\n",
        "    \"n_fires\",\n",
        "    \"area_burned\",\n",
        "    \"any_fire\",\n",
        "]\n",
        "for c in fire_cols:\n",
        "    if c in data.columns:\n",
        "        data[c] = data[c].fillna(0)\n",
        "\n",
        "# Add transformed target for area\n",
        "data[\"log_area_burned\"] = np.log1p(data[\"area_burned\"])\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "6DR2Cl2lT66C",
        "outputId": "3e08fb3d-9379-4487-915b-e9b4fb7d0ed7"
      },
      "outputs": [],
      "source": [
        "# ---- 4.1 Choose features for clustering ----\n",
        "cluster_features = [\n",
        "    \"tavg_mean_7\",\n",
        "    \"tavg_mean_30\",\n",
        "    \"prcp_sum_7\",\n",
        "    \"prcp_sum_30\",\n",
        "    \"wspd_mean_7\",\n",
        "    \"pres_mean_7\",\n",
        "]\n",
        "\n",
        "# Some columns may not exist depending on your base_cols / windows;\n",
        "# filter to those that actually exist.\n",
        "cluster_features = [c for c in cluster_features if c in data.columns]\n",
        "\n",
        "# ---- 4.2 Fit KMeans on training-years only ----\n",
        "train_year_cutoff = 2011\n",
        "train_mask = data[\"year\"] < train_year_cutoff\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_cluster_train = scaler.fit_transform(data.loc[train_mask, cluster_features])\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=RANDOM_STATE, n_init=10)\n",
        "kmeans.fit(X_cluster_train)\n",
        "\n",
        "# Assign cluster to all rows\n",
        "data[\"weather_cluster\"] = kmeans.predict(\n",
        "    scaler.transform(data[cluster_features])\n",
        ")\n",
        "\n",
        "data[\"weather_cluster\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW1XF4aRT9FN",
        "outputId": "0798f252-1d32-4f53-c880-780a698304f2"
      },
      "outputs": [],
      "source": [
        "# Label-encode states\n",
        "le_state = LabelEncoder()\n",
        "data[\"STATE_LE\"] = le_state.fit_transform(data[\"STATE\"])\n",
        "\n",
        "# Final feature list (excluding targets)\n",
        "exclude_cols = {\n",
        "    \"n_fires\",\n",
        "    \"any_fire\",\n",
        "    \"area_burned\",\n",
        "    \"log_area_burned\",\n",
        "    \"STATE\",\n",
        "    \"date\",\n",
        "} | set([c for c in data.columns if c.startswith(\"cause_\")])\n",
        "\n",
        "feature_cols = [c for c in data.columns if c not in exclude_cols]\n",
        "len(feature_cols), feature_cols[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fmH6UQWV1Mo"
      },
      "outputs": [],
      "source": [
        "# Train / test split by year (no leakage from future to past)\n",
        "train_mask = data[\"year\"] < train_year_cutoff\n",
        "test_mask = ~train_mask\n",
        "\n",
        "X_train = data.loc[train_mask, feature_cols]\n",
        "X_test = data.loc[test_mask, feature_cols]\n",
        "\n",
        "y_train_count = data.loc[train_mask, \"n_fires\"]\n",
        "y_test_count = data.loc[test_mask, \"n_fires\"]\n",
        "\n",
        "y_train_bin = data.loc[train_mask, \"any_fire\"]\n",
        "y_test_bin = data.loc[test_mask, \"any_fire\"]\n",
        "\n",
        "y_train_log_area = data.loc[train_mask, \"log_area_burned\"]\n",
        "y_test_log_area = data.loc[test_mask, \"log_area_burned\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82olVROAT-vL",
        "outputId": "183a9e2b-de66-43a7-c228-d181d84919c5"
      },
      "outputs": [],
      "source": [
        "# ---- 6.1 Count model (Poisson) ----\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train, label=y_train_count)\n",
        "lgb_valid = lgb.Dataset(X_test, label=y_test_count)\n",
        "\n",
        "params_count = {\n",
        "    \"objective\": \"poisson\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 64,\n",
        "    \"metric\": \"rmse\",\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"seed\": RANDOM_STATE,\n",
        "}\n",
        "\n",
        "model_count = lgb.train(\n",
        "    params_count,\n",
        "    lgb_train,\n",
        "    num_boost_round=2000,\n",
        "    valid_sets=[lgb_valid],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
        ")\n",
        "\n",
        "pred_count = model_count.predict(X_test, num_iteration=model_count.best_iteration)\n",
        "# Fix: Remove squared=False and take np.sqrt to get RMSE\n",
        "rmse_count = np.sqrt(mean_squared_error(y_test_count, pred_count))\n",
        "\n",
        "print(f\"Count model RMSE: {rmse_count:.3f}\")\n",
        "print(f\"Mean actual count in test: {y_test_count.mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIVl75KNjQ67",
        "outputId": "d5f4bc7c-8db7-4524-cb27-8eeb31fe96d7"
      },
      "outputs": [],
      "source": [
        "# ---- 6.2 Binary any-fire model (for AUC) ----\n",
        "\n",
        "params_bin = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 64,\n",
        "    \"metric\": \"auc\",\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"seed\": RANDOM_STATE,\n",
        "}\n",
        "\n",
        "lgb_train_bin = lgb.Dataset(X_train, label=y_train_bin)\n",
        "lgb_valid_bin = lgb.Dataset(X_test, label=y_test_bin)\n",
        "\n",
        "model_bin = lgb.train(\n",
        "    params_bin,\n",
        "    lgb_train_bin,\n",
        "    num_boost_round=2000,\n",
        "    valid_sets=[lgb_valid_bin],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
        ")\n",
        "\n",
        "prob_any_fire = model_bin.predict(\n",
        "    X_test, num_iteration=model_bin.best_iteration\n",
        ")\n",
        "auc_any_fire = roc_auc_score(y_test_bin, prob_any_fire)\n",
        "\n",
        "print(f\"AUC (any_fire): {auc_any_fire:.3f}\")\n",
        "print(f\"Positive rate (fires on a day): {y_test_bin.mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCxTQ701jsLe",
        "outputId": "29aed757-3449-4abc-9eec-478d2189dc8c"
      },
      "outputs": [],
      "source": [
        "# ---- 6.3 Simple clustering-only baseline for counts ----\n",
        "# Predict the mean count for each weather_cluster on train,\n",
        "# then apply to test based only on cluster label.\n",
        "\n",
        "cluster_means = (\n",
        "    data.loc[train_mask]\n",
        "    .groupby(\"weather_cluster\")[\"n_fires\"]\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "baseline_pred = data.loc[test_mask, \"weather_cluster\"].map(cluster_means)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(\n",
        "    y_test_count, baseline_pred\n",
        "))\n",
        "\n",
        "print(f\"Cluster baseline RMSE: {baseline_rmse:.3f}\")\n",
        "print(f\"LightGBM Poisson RMSE: {rmse_count:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnLtj0N9YHrM",
        "outputId": "893d03ba-57d0-4f5f-d1c2-50ca33fa14c6"
      },
      "outputs": [],
      "source": [
        "# ---- 7.1 Area model ----\n",
        "\n",
        "params_area = {\n",
        "    \"objective\": \"regression\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 64,\n",
        "    \"metric\": \"rmse\",\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"seed\": RANDOM_STATE,\n",
        "}\n",
        "\n",
        "lgb_train_area = lgb.Dataset(X_train, label=y_train_log_area)\n",
        "lgb_valid_area = lgb.Dataset(X_test, label=y_test_log_area)\n",
        "\n",
        "model_area = lgb.train(\n",
        "    params_area,\n",
        "    lgb_train_area,\n",
        "    num_boost_round=2000,\n",
        "    valid_sets=[lgb_valid_area],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
        ")\n",
        "\n",
        "pred_log_area = model_area.predict(\n",
        "    X_test, num_iteration=model_area.best_iteration\n",
        ")\n",
        "\n",
        "rmse_log_area = np.sqrt(mean_squared_error(\n",
        "    y_test_log_area, pred_log_area\n",
        "))\n",
        "r2_log_area = r2_score(y_test_log_area, pred_log_area)\n",
        "\n",
        "print(f\"Area model RMSE (log1p): {rmse_log_area:.3f}\")\n",
        "print(f\"Area model R^2 (log1p): {r2_log_area:.3f}\")\n",
        "\n",
        "# Baseline: predict mean log-area of train\n",
        "baseline_log = np.full_like(y_test_log_area, y_train_log_area.mean())\n",
        "baseline_rmse_log = np.sqrt(mean_squared_error(\n",
        "    y_test_log_area, baseline_log\n",
        "))\n",
        "print(f\"Baseline RMSE (log1p mean): {baseline_rmse_log:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrwwJfcVkvry"
      },
      "outputs": [],
      "source": [
        "# ---- 8.1 Prepare cause targets ----\n",
        "\n",
        "cause_cols = [c for c in data.columns if c.startswith(\"cause_\")]\n",
        "len(cause_cols), cause_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3zT_X1zkxuM"
      },
      "outputs": [],
      "source": [
        "# Keep only days with at least one fire\n",
        "data_cause = data[data[\"n_fires\"] > 0].copy()\n",
        "\n",
        "# Proportions per cause\n",
        "for c in cause_cols:\n",
        "    data_cause[c + \"_prop\"] = data_cause[c] / data_cause[\"n_fires\"]\n",
        "\n",
        "target_prop_cols = [c + \"_prop\" for c in cause_cols]\n",
        "\n",
        "# Train/test split by year (same cutoff)\n",
        "train_mask_cause = data_cause[\"year\"] < train_year_cutoff\n",
        "test_mask_cause = ~train_mask_cause\n",
        "\n",
        "X_train_cause = data_cause.loc[train_mask_cause, feature_cols]\n",
        "X_test_cause = data_cause.loc[test_mask_cause, feature_cols]\n",
        "\n",
        "Y_train_cause = data_cause.loc[train_mask_cause, target_prop_cols].values\n",
        "Y_test_cause = data_cause.loc[test_mask_cause, target_prop_cols].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4018c8a2"
      },
      "outputs": [],
      "source": [
        "# Load only STAT_CAUSE_CODE and STAT_CAUSE_DESCR to create a mapping\n",
        "# fires_path is already defined in the notebook.\n",
        "fires_cause_descriptions = pd.read_parquet(\n",
        "    fires_path,\n",
        "    columns=[\"STAT_CAUSE_CODE\", \"STAT_CAUSE_DESCR\"]\n",
        ")\n",
        "\n",
        "# Create a unique mapping from code to description\n",
        "# Handle potential duplicates by taking the first description for each code\n",
        "cause_code_to_descr = fires_cause_descriptions[['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']].drop_duplicates().set_index('STAT_CAUSE_CODE')['STAT_CAUSE_DESCR'].to_dict()\n",
        "\n",
        "print(\"Cause Code to Description Mapping:\")\n",
        "for code, descr in sorted(cause_code_to_descr.items()):\n",
        "    print(f\"{code}: {descr}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8694040c"
      },
      "outputs": [],
      "source": [
        "# Prepare the renaming dictionary\n",
        "rename_dict = {}\n",
        "for col in data.columns:\n",
        "    if col.startswith(\"cause_\"):\n",
        "        cause_code_str = col.split('_')[1] # Extract the numeric code as string\n",
        "        # Check if the extracted string is actually a digit before converting to int\n",
        "        if cause_code_str.isdigit():\n",
        "            cause_code = int(cause_code_str)\n",
        "            if cause_code in cause_code_to_descr:\n",
        "                # Create a clean, descriptive column name\n",
        "                # Replace spaces/special chars with underscores and convert to lowercase\n",
        "                new_col_name = \"cause_\" + cause_code_to_descr[cause_code].replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\"(\",\"\").replace(\")\",\"\").lower()\n",
        "                rename_dict[col] = new_col_name\n",
        "            else:\n",
        "                print(f\"Warning: No description found for STAT_CAUSE_CODE {cause_code}. Column '{col}' will not be renamed.\")\n",
        "        else:\n",
        "            # If it's not a digit, it's likely already renamed or not a standard numeric cause column.\n",
        "            # We can skip renaming it.\n",
        "            print(f\"Info: Column '{col}' is not in 'cause_X' numeric format; skipping renaming.\")\n",
        "\n",
        "# Apply the renaming to the DataFrame\n",
        "data = data.rename(columns=rename_dict)\n",
        "\n",
        "# Display the head of the DataFrame to show the new column names\n",
        "print(\"DataFrame with renamed cause columns:\")\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "574d6fdf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_train_log_area, bins=50, kde=True)\n",
        "plt.title('Distribution of log_area_burned (Training Set) - Recheck')\n",
        "plt.xlabel('log(Area Burned + 1)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceea1407"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_test_log_area, bins=50, kde=True)\n",
        "plt.title('Distribution of log_area_burned (Test Set) - Recheck')\n",
        "plt.xlabel('log(Area Burned + 1)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72e32ed5"
      },
      "outputs": [],
      "source": [
        "print('Summary statistics for y_train_log_area (Recheck):')\n",
        "print(y_train_log_area.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f01d7b7a"
      },
      "outputs": [],
      "source": [
        "print('\\nSummary statistics for y_test_log_area (Recheck):')\n",
        "print(y_test_log_area.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a639178f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_train_log_area, bins=50, kde=True)\n",
        "plt.title('Distribution of log_area_burned (Training Set)')\n",
        "plt.xlabel('log(Area Burned + 1)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d22c047d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_test_log_area, bins=50, kde=True)\n",
        "plt.title('Distribution of log_area_burned (Test Set)')\n",
        "plt.xlabel('log(Area Burned + 1)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cff2cb4"
      },
      "outputs": [],
      "source": [
        "print('Summary statistics for y_train_log_area:')\n",
        "print(y_train_log_area.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7421eef"
      },
      "outputs": [],
      "source": [
        "print('\\nSummary statistics for y_test_log_area:')\n",
        "print(y_test_log_area.describe())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
