{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995738a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    r2_score,\n",
    "    silhouette_score,\n",
    "),\n",
    "\n",
    "import holidays\n",
    "\n",
    "DATA_DIR = Path('shared_data/')\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddab4be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'shared_data\\\\wildfires_fires.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---- 1.1 Load wildfires ----\u001b[39;00m\n\u001b[32m      2\u001b[39m fires_path = DATA_DIR / \u001b[33m'\u001b[39m\u001b[33mwildfires_fires.parquet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m fires = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfires_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFIRE_SIZE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDISCOVERY_DATE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSTATE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSTAT_CAUSE_CODE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.issubdtype(fires[\u001b[33m'\u001b[39m\u001b[33mDISCOVERY_DATE\u001b[39m\u001b[33m'\u001b[39m].dtype, np.datetime64):\n\u001b[32m      9\u001b[39m     fires[\u001b[33m'\u001b[39m\u001b[33mDISCOVERY_DATE\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(fires[\u001b[33m'\u001b[39m\u001b[33mDISCOVERY_DATE\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SPAC-18\\wildfires_vsc\\wildfire_prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SPAC-18\\wildfires_vsc\\wildfire_prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SPAC-18\\wildfires_vsc\\wildfire_prediction\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SPAC-18\\wildfires_vsc\\wildfire_prediction\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'shared_data\\\\wildfires_fires.parquet'"
     ]
    }
   ],
   "source": [
    "# ---- 1.1 Load wildfires ----\n",
    "fires_path = DATA_DIR / 'wildfires_fires.parquet'\n",
    "fires = pd.read_parquet(\n",
    "    fires_path,\n",
    "    columns=['FIRE_SIZE', 'DISCOVERY_DATE', 'STATE', 'STAT_CAUSE_CODE'],\n",
    ")\n",
    "\n",
    "if not np.issubdtype(fires['DISCOVERY_DATE'].dtype, np.datetime64):\n",
    "    fires['DISCOVERY_DATE'] = pd.to_datetime(fires['DISCOVERY_DATE'])\n",
    "\n",
    "fires = fires.rename(columns={'DISCOVERY_DATE': 'date'})\n",
    "fires = fires[(fires['date'].dt.year >= 1992) & (fires['date'].dt.year <= 2015)]\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.2 Aggregate to state–day ----\n",
    "daily_agg = (\n",
    "    fires.groupby(['STATE', 'date'])\n",
    "    .agg(\n",
    "        n_fires=('FIRE_SIZE', 'size'),\n",
    "        area_burned=('FIRE_SIZE', 'sum'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cause_counts = (\n",
    "    fires.pivot_table(\n",
    "        index=['STATE', 'date'],\n",
    "        columns='STAT_CAUSE_CODE',\n",
    "        values='FIRE_SIZE',\n",
    "        aggfunc='count',\n",
    "        fill_value=0,\n",
    "    )\n",
    "    .rename_axis(columns='cause_code')\n",
    ")\n",
    "cause_counts.columns = [f'cause_{int(c)}' for c in cause_counts.columns]\n",
    "cause_counts = cause_counts.reset_index()\n",
    "\n",
    "fires_daily = daily_agg.merge(cause_counts, on=['STATE', 'date'], how='left').fillna(0)\n",
    "fires_daily['any_fire'] = (fires_daily['n_fires'] > 0).astype(int)\n",
    "fires_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f70cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.1 Load weather ----\n",
    "weather_path = DATA_DIR / 'us_daily_weather_1992_2015.parquet'\n",
    "weather = pd.read_parquet(weather_path)\n",
    "\n",
    "cities_path = DATA_DIR / 'cities.csv'\n",
    "cities_df = pd.read_csv(cities_path)\n",
    "\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "weather = weather[(weather['date'].dt.year >= 1992) & (weather['date'].dt.year <= 2015)]\n",
    "\n",
    "weather = weather.merge(\n",
    "    cities_df[['city_name', 'state']].drop_duplicates(), on='city_name', how='left'\n",
    ")\n",
    "\n",
    "weather = weather.rename(columns={\n",
    "    'state': 'STATE',\n",
    "    'avg_temp_c': 'tavg',\n",
    "    'min_temp_c': 'tmin',\n",
    "    'max_temp_c': 'tmax',\n",
    "    'precipitation_mm': 'prcp',\n",
    "    'snow_depth_mm': 'snow',\n",
    "    'avg_wind_speed_kmh': 'wspd',\n",
    "    'avg_sea_level_pres_hpa': 'pres',\n",
    "})\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50fed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.2 Aggregate station data to state–day ----\n",
    "weather_daily = (\n",
    "    weather.groupby(['STATE', 'date'])\n",
    "    .agg(\n",
    "        tavg=('tavg', 'mean'),\n",
    "        tmin=('tmin', 'mean'),\n",
    "        tmax=('tmax', 'mean'),\n",
    "        prcp=('prcp', 'sum'),\n",
    "        snow=('snow', 'sum'),\n",
    "        wspd=('wspd', 'mean'),\n",
    "        pres=('pres', 'mean'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "weather_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6235c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.3 Lagged features helper ----\n",
    "def add_lagged_features(df, group_col, date_col, base_cols, windows=(3, 7, 30), shift=1):\n",
    "    df = df.sort_values([group_col, date_col]).copy()\n",
    "    for col in base_cols:\n",
    "        for w in windows:\n",
    "            roll = (\n",
    "                df.groupby(group_col)[col]\n",
    "                .transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
    "            )\n",
    "            df[f'{col}_mean_{w}'] = roll.shift(shift)\n",
    "    for w in [7, 30]:\n",
    "        roll = (\n",
    "            df.groupby(group_col)['prcp']\n",
    "            .transform(lambda x: x.rolling(w, min_periods=1).sum())\n",
    "        )\n",
    "        df[f'prcp_sum_{w}'] = roll.shift(shift)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e749d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.4 Build weather features with state mapping and lags ----\n",
    "weather_feat = weather_daily.copy()\n",
    "\n",
    "state_name_to_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
    "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
    "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "        \n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "        \n",
    "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "        \n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "        \n",
    "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "        \n",
    "\n",
    "weather_feat['STATE'] = weather_feat['STATE'].map(state_name_to_abbrev)\n",
    "weather_feat.dropna(subset=['STATE'], inplace=True)\n",
    "\n",
    "weather_feat = weather_feat.set_index('STATE')\n",
    "weather_feat = weather_feat.groupby(level=0).ffill().bfill()\n",
    "weather_feat = weather_feat.reset_index()\n",
    "\n",
    "weather_feat['year'] = weather_feat['date'].dt.year\n",
    "weather_feat['month'] = weather_feat['date'].dt.month\n",
    "weather_feat['doy'] = weather_feat['date'].dt.dayofyear\n",
    "\n",
    "base_cols = ['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'pres']\n",
    "weather_feat = add_lagged_features(\n",
    "    weather_feat, group_col='STATE', date_col='date', base_cols=base_cols\n",
    ")\n",
    "weather_feat = weather_feat.dropna().reset_index(drop=True)\n",
    "weather_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab970e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3.1 Merge with fires and add targets + calendar / holiday features ----\n",
    "data = weather_feat.merge(fires_daily, on=['STATE', 'date'], how='left')\n",
    "\n",
    "fire_cols = [c for c in data.columns if c.startswith('cause_')] + [\n",
    "    'n_fires', 'area_burned', 'any_fire',\n",
    "]\n",
    "for c in fire_cols:\n",
    "    if c in data.columns:\n",
    "        data[c] = data[c].fillna(0)\n",
    "\n",
    "data['log_area_burned'] = np.log1p(data['area_burned'])\n",
    "\n",
    "# Weekend indicator\n",
    "data['is_weekend'] = data['date'].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "# US holidays and July 4th indicator\n",
    "us_holidays = holidays.US()\n",
    "data['holiday_name'] = data['date'].dt.date.map(us_holidays.get)\n",
    "data['is_holiday'] = data['holiday_name'].notna().astype(int)\n",
    "data['is_july4'] = (data['holiday_name'] == 'Independence Day').astype(int)\n",
    "\n",
    "# Previous-day precipitation feature (per state)\n",
    "data = data.sort_values(['STATE', 'date'])\n",
    "data['prcp_prev_1d'] = data.groupby('STATE')['prcp'].shift(1)\n",
    "data['prcp_prev_1d'] = data['prcp_prev_1d'].fillna(0.0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1716fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4.1 Choose features for clustering (including calendar & prev-day precipitation) ----\n",
    "cluster_features = [\n",
    "    'tavg_mean_7',\n",
    "    'tavg_mean_30',\n",
    "    'prcp_sum_7',\n",
    "    'prcp_sum_30',\n",
    "    'wspd_mean_7',\n",
    "    'pres_mean_7',\n",
    "    # Calendar / holiday / previous-day precipitation features\n",
    "    'prcp_prev_1d',\n",
    "    'is_weekend',\n",
    "    'is_holiday',\n",
    "    'is_july4',\n",
    "]\n",
    "cluster_features = [c for c in cluster_features if c in data.columns]\n",
    "\n",
    "train_year_cutoff = 2011\n",
    "train_mask = data['year'] < train_year_cutoff\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_cluster_train = scaler.fit_transform(data.loc[train_mask, cluster_features])\n",
    "X_cluster_full = scaler.transform(data[cluster_features])\n",
    "\n",
    "# ---- 4.2 PCA dimensionality reduction ----\n",
    "# Keep enough components to explain ~95% of variance\n",
    "pca = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
    "X_cluster_train_pca = pca.fit_transform(X_cluster_train)\n",
    "X_cluster_full_pca = pca.transform(X_cluster_full)\n",
    "\n",
    "# ---- 4.3 KMeans with small automatic K search (in PCA space) ----\n",
    "candidate_ks = [3, 5, 8, 10]\n",
    "best_k = None\n",
    "best_score = -1.0\n",
    "best_model = None\n",
    "\n",
    "for k in candidate_ks:\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = km.fit_predict(X_cluster_train_pca)\n",
    "    # Silhouette needs at least 2 clusters\n",
    "    if len(set(labels)) < 2:\n",
    "        continue\n",
    "    score = silhouette_score(X_cluster_train_pca, labels)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "        best_model = km\n",
    "\n",
    "print(f\"Chosen K for KMeans (PCA space): {best_k}, silhouette={best_score:.3f}\")\n",
    "\n",
    "kmeans = best_model\n",
    "data['weather_cluster_kmeans'] = kmeans.predict(X_cluster_full_pca)\n",
    "\n",
    "# ---- 4.4 DBSCAN in PCA space with simple eps heuristic ----\n",
    "eps_candidates = [0.3, 0.5, 0.8, 1.0]\n",
    "min_samples = 50\n",
    "\n",
    "best_eps = None\n",
    "best_db = None\n",
    "best_n_clusters = -1\n",
    "\n",
    "for eps in eps_candidates:\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(X_cluster_train_pca)\n",
    "    unique_labels = set(labels) - {-1}\n",
    "    n_clusters = len(unique_labels)\n",
    "    # Require at least 2 clusters and not too many\n",
    "    if n_clusters < 2 or n_clusters > 30:\n",
    "        continue\n",
    "    if n_clusters > best_n_clusters:\n",
    "        best_n_clusters = n_clusters\n",
    "        best_eps = eps\n",
    "        best_db = db\n",
    "\n",
    "if best_db is None:\n",
    "    # Fallback DBSCAN if no candidate passes the heuristic\n",
    "    best_eps = 0.8\n",
    "    best_db = DBSCAN(eps=best_eps, min_samples=min_samples).fit(X_cluster_train_pca)\n",
    "\n",
    "print(f\"Chosen eps for DBSCAN (PCA space): {best_eps}, clusters={best_n_clusters}\")\n",
    "\n",
    "dbscan = best_db\n",
    "data['weather_cluster_dbscan'] = dbscan.fit_predict(X_cluster_full_pca)\n",
    "\n",
    "data['weather_cluster_kmeans'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5.1 Final feature matrix and splits ----\n",
    "le_state = LabelEncoder()\n",
    "data['STATE_LE'] = le_state.fit_transform(data['STATE'])\n",
    "\n",
    "exclude_cols = {\n",
    "    'n_fires', 'any_fire', 'area_burned', 'log_area_burned', 'STATE', 'date'\n",
    "} | set([c for c in data.columns if c.startswith('cause_')])\n",
    "\n",
    "feature_cols = [c for c in data.columns if c not in exclude_cols]\n",
    "\n",
    "train_mask = data['year'] < train_year_cutoff\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_train = data.loc[train_mask, feature_cols]\n",
    "X_test = data.loc[test_mask, feature_cols]\n",
    "\n",
    "y_train_count = data.loc[train_mask, 'n_fires']\n",
    "y_test_count = data.loc[test_mask, 'n_fires']\n",
    "y_train_bin = data.loc[train_mask, 'any_fire']\n",
    "y_test_bin = data.loc[test_mask, 'any_fire']\n",
    "y_train_log_area = data.loc[train_mask, 'log_area_burned']\n",
    "y_test_log_area = data.loc[test_mask, 'log_area_burned']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
