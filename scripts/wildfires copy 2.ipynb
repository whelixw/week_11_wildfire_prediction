{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce95e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "import holidays\n",
    "\n",
    "DATA_DIR = Path('../shared_data/')  # change to your local path\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ab33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.1 Load wildfires ----\n",
    "fires_path = DATA_DIR / 'wildfires_fires.parquet'\n",
    "\n",
    "fires = pd.read_parquet(\n",
    "    fires_path,\n",
    "    columns=[\n",
    "        'FIRE_SIZE',\n",
    "        'DISCOVERY_DATE',\n",
    "        'STATE',\n",
    "        'STAT_CAUSE_CODE',\n",
    "        'LATITUDE',\n",
    "        'LONGITUDE',\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Convert DISCOVERY_DATE to datetime\n",
    "if not np.issubdtype(fires['DISCOVERY_DATE'].dtype, np.datetime64):\n",
    "    fires['DISCOVERY_DATE'] = pd.to_datetime(fires['DISCOVERY_DATE'])\n",
    "\n",
    "fires = fires.rename(columns={'DISCOVERY_DATE': 'date'})\n",
    "\n",
    "# Filter date range 1992–2015\n",
    "fires = fires[\n",
    "    (fires['date'].dt.year >= 1992) & (fires['date'].dt.year <= 2015)\n",
    "]\n",
    "\n",
    "# Drop rows without coordinates\n",
    "fires = fires.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df6f7f",
   "metadata": {},
   "source": [
    "# 0. Exploratory data analysis (EDA)\n",
    "\n",
    "\n",
    "In this section we explore the raw wildfire and station-mapped data before building models.\n",
    "\n",
    "\n",
    "We focus on:\n",
    "\n",
    "- The distribution of distances from each fire to its nearest weather station.\n",
    "\n",
    "- The distribution of burned area per fire (on both raw and log scales).\n",
    "\n",
    "- Basic counts per state and over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f70e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45dc9ab0",
   "metadata": {},
   "source": [
    "The log-fire size has a bit of an erratic distribution, the spikes are likely due to rounding. We should explore this further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b963d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.1b Attach closest weather station to each fire ----\n",
    "cities_path = DATA_DIR / 'cities.csv'\n",
    "stations = pd.read_csv(cities_path)\n",
    "\n",
    "# Keep one row per station with coordinates\n",
    "stations = stations[[\"station_id\", \"latitude\", \"longitude\"]].drop_duplicates('station_id')\n",
    "\n",
    "# Build BallTree (coordinates in radians)\n",
    "station_coords = np.radians(stations[['latitude', 'longitude']].values)\n",
    "fire_coords = np.radians(fires[['LATITUDE', 'LONGITUDE']].values)\n",
    "\n",
    "tree = BallTree(station_coords, metric='haversine')\n",
    "dist_rad, ind = tree.query(fire_coords, k=1)\n",
    "\n",
    "fires['station_id'] = stations.iloc[ind.flatten()]['station_id'].values\n",
    "fires['dist_to_station_km'] = dist_rad.flatten() * 6371.0  # earth radius ~6371 km\n",
    "\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e326011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 0.1 EDA: distance to nearest station and burned area ----\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure we have the station distance in km (computed in the previous cell)\n",
    "assert 'dist_to_station_km' in fires.columns, 'dist_to_station_km missing; run the station mapping cell first.'\n",
    "\n",
    "# Distribution of distance to nearest station\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(fires['dist_to_station_km'], bins=50, kde=True)\n",
    "plt.xlabel('Distance to nearest station (km)')\n",
    "plt.title('Distribution of distance from fire to nearest station')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Raw area burned per fire\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(fires['FIRE_SIZE'], bins=100, log_scale=(False, True))\n",
    "plt.xlabel('FIRE_SIZE (area burned)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Distribution of raw FIRE_SIZE (per fire)')\n",
    "\n",
    "# Log-transformed area per fire\n",
    "plt.subplot(1, 2, 2)\n",
    "log_fire_size = np.log1p(fires['FIRE_SIZE'])\n",
    "sns.histplot(log_fire_size, bins=100, kde=True)\n",
    "plt.xlabel('log1p(FIRE_SIZE)')\n",
    "plt.title('Distribution of log1p(FIRE_SIZE) (per fire)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# EDA: simple counts per state and over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "fires['STATE'].value_counts().head(15).plot(kind='bar')\n",
    "plt.title('Top 15 states by number of recorded fires')\n",
    "plt.xlabel('STATE')\n",
    "plt.ylabel('Number of fires')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "fires['date'].dt.year.value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Number of fires per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of fires')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.1c Create fire-level distance-pruned versions ----\n",
    "# We will create three variants of the raw fire table where we drop fires\n",
    "# whose nearest-station distance is below a chosen cutoff (100, 200, 400 km).\n",
    "# NOTE: pruning is done at the **fire-level**, before any aggregation.\n",
    "\n",
    "cutoffs_km = [100, 200, 400]\n",
    "fires_pruned = {}\n",
    "\n",
    "for d in cutoffs_km:\n",
    "    mask = fires['dist_to_station_km'] <= d\n",
    "    fires_pruned[d] = fires.loc[mask].copy()\n",
    "    print(f\"Cutoff {d} km: keeping {mask.sum():,} fires out of {len(fires):,} total\")\n",
    "\n",
    "# Keep the original (unpruned) fires table for reference as well\n",
    "fires_pruned[0] = fires.copy()  # 0 km = no pruning\n",
    "print(\"Stored distance-pruned fire tables in fires_pruned dict (keys: all, 100, 200, 400)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55dfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 0.1 EDA: distance to nearest station and burned area ----\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure we have the station distance in km (computed in the previous cell)\n",
    "assert 'dist_to_station_km' in fires.columns, 'dist_to_station_km missing; run the station mapping cell first.'\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(fires['dist_to_station_km'], bins=50, kde=True)\n",
    "plt.xlabel('Distance to nearest station (km)')\n",
    "plt.title('Distribution of distance from fire to nearest station')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Raw area burned per fire\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(fires['FIRE_SIZE'], bins=100, log_scale=(False, True))\n",
    "plt.xlabel('FIRE_SIZE (area burned)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Distribution of raw FIRE_SIZE (per fire)')\n",
    "\n",
    "# Log-transformed area per fire\n",
    "plt.subplot(1, 2, 2)\n",
    "log_fire_size = np.log1p(fires['FIRE_SIZE'])\n",
    "sns.histplot(log_fire_size, bins=100, kde=True)\n",
    "plt.xlabel('log1p(FIRE_SIZE)')\n",
    "plt.title('Distribution of log1p(FIRE_SIZE) (per fire)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# EDA: simple counts per state and over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "fires['STATE'].value_counts().head(15).plot(kind='bar')\n",
    "plt.title('Top 15 states by number of recorded fires')\n",
    "plt.xlabel('STATE')\n",
    "plt.ylabel('Number of fires')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "fires['date'].dt.year.value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Number of fires per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of fires')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55dfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.2 Aggregate to station–day ----\n",
    "\n",
    "# Optionally: restrict to fires within a maximum distance of their assigned station\n",
    "\n",
    "# Set this to None to keep all fires, or to a value in km (e.g., 400) to drop very distant matches.\n",
    "max_fire_station_distance_km = None  # e.g., 400\n",
    "fires_for_agg = fires.copy()\n",
    "if max_fire_station_distance_km is not None:\n",
    "    fires_for_agg = fires_for_agg[fires_for_agg['dist_to_station_km'] <= max_fire_station_distance_km].copy()\n",
    "    print(f\"Using {len(fires_for_agg):,} fires within {max_fire_station_distance_km} km of a station (out of {len(fires):,})\")\n",
    "\n",
    "# Basic daily targets per station\n",
    "daily_agg = (\n",
    "    fires_for_agg.groupby(['station_id', 'date'])\n",
    "    .agg(\n",
    "        n_fires=('FIRE_SIZE', 'size'),\n",
    "        area_burned=('FIRE_SIZE', 'sum'),\n",
    "        min_dist_km=('dist_to_station_km', 'min'),\n",
    "        mean_dist_km=('dist_to_station_km', 'mean'),\n",
    "    )\n",
    "    .reset_index()\n",
    " )\n",
    "\n",
    "# Cause counts per station–day (wide format)\n",
    "cause_counts = (\n",
    "    fires_for_agg.pivot_table(\n",
    "        index=['station_id', 'date'],\n",
    "        columns='STAT_CAUSE_CODE',\n",
    "        values='FIRE_SIZE',  # any column, we just count\n",
    "        aggfunc='count',\n",
    "        fill_value=0,\n",
    "    )\n",
    "    .rename_axis(columns='cause_code')\n",
    ")\n",
    "\n",
    "# Make nicer column names: cause_1, cause_2, ...\n",
    "cause_counts.columns = [f'cause_{int(c)}' for c in cause_counts.columns]\n",
    "cause_counts = cause_counts.reset_index()\n",
    "\n",
    "# Merge counts into daily_agg\n",
    "fires_daily = daily_agg.merge(\n",
    "    cause_counts, on=['station_id', 'date'], how='left',\n",
    ").fillna(0)\n",
    "\n",
    "fires_daily['any_fire'] = (fires_daily['n_fires'] > 0).astype(int)\n",
    "fires_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a47950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.2b Aggregate pruned fires to station–day ----\n",
    "# For each distance cutoff, build a station–day table analogous to `fires_daily`.\n",
    "\n",
    "fires_daily_by_cutoff = {}\n",
    "\n",
    "for d, fires_subset in fires_pruned.items():\n",
    "    # Optionally mirror the max_fire_station_distance_km filter logic if desired\n",
    "    fires_for_agg_d = fires_subset.copy()\n",
    "    if max_fire_station_distance_km is not None:\n",
    "        fires_for_agg_d = fires_for_agg_d[\n",
    "            fires_for_agg_d['dist_to_station_km'] <= max_fire_station_distance_km\n",
    "        ].copy()\n",
    "\n",
    "    if fires_for_agg_d.empty:\n",
    "        print(f\"Warning: no fires left after filtering for cutoff {d} km; skipping.\")\n",
    "        continue\n",
    "\n",
    "    daily_agg_d = (\n",
    "        fires_for_agg_d.groupby(['station_id', 'date'])\n",
    "        .agg(\n",
    "            n_fires=('FIRE_SIZE', 'size'),\n",
    "            area_burned=('FIRE_SIZE', 'sum'),\n",
    "            min_dist_km=('dist_to_station_km', 'min'),\n",
    "            mean_dist_km=('dist_to_station_km', 'mean'),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    cause_counts_d = (\n",
    "        fires_for_agg_d.pivot_table(\n",
    "            index=['station_id', 'date'],\n",
    "            columns='STAT_CAUSE_CODE',\n",
    "            values='FIRE_SIZE',\n",
    "            aggfunc='count',\n",
    "            fill_value=0,\n",
    "        )\n",
    "        .rename_axis(columns='cause_code')\n",
    "    )\n",
    "    cause_counts_d.columns = [f'cause_{int(c)}' for c in cause_counts_d.columns]\n",
    "    cause_counts_d = cause_counts_d.reset_index()\n",
    "\n",
    "    fires_daily_d = daily_agg_d.merge(\n",
    "        cause_counts_d, on=['station_id', 'date'], how='left'\n",
    "    ).fillna(0)\n",
    "    fires_daily_d['any_fire'] = (fires_daily_d['n_fires'] > 0).astype(int)\n",
    "\n",
    "    fires_daily_by_cutoff[d] = fires_daily_d\n",
    "    print(\n",
    "        f\"Cutoff {d} km: {len(fires_daily_d):,} station-days with at least one fire event (post-aggregation)\"\n",
    "    )\n",
    "\n",
    "# For backward compatibility, keep `fires_daily` as the unpruned (d=0) version if not already\n",
    "if 0 in fires_daily_by_cutoff:\n",
    "    fires_daily = fires_daily_by_cutoff[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50460dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.1 Load weather (station-based) ----\n",
    "weather_path = DATA_DIR / 'us_daily_weather_1992_2015.parquet'\n",
    "weather = pd.read_parquet(weather_path)\n",
    "\n",
    "# Ensure date is datetime\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "\n",
    "# Restrict to 1992–2015\n",
    "weather = weather[\n",
    "    (weather['date'].dt.year >= 1992) & (weather['date'].dt.year <= 2015)\n",
    "]\n",
    "\n",
    "# Load station metadata (cities.csv)\n",
    "cities_path = DATA_DIR / 'cities.csv'\n",
    "cities_df = pd.read_csv(cities_path)\n",
    "\n",
    "# Merge state info into weather via station_id\n",
    "weather = weather.merge(\n",
    "    cities_df[['station_id', 'state']].drop_duplicates('station_id'),\n",
    "    on='station_id',\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "weather = weather.rename(\n",
    "    columns={\n",
    "        'state': 'STATE',\n",
    "        'avg_temp_c': 'tavg',\n",
    "        'min_temp_c': 'tmin',\n",
    "        'max_temp_c': 'tmax',\n",
    "        'precipitation_mm': 'prcp_1d',  # explicit 1-day precipitation\n",
    "        'avg_wind_speed_kmh': 'wspd',\n",
    "        'avg_sea_level_pres_hpa': 'pres',\n",
    "    }\n",
    ")\n",
    "\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.2 Aggregate station data to station–day ----\n",
    "weather_daily = (\n",
    "    weather.groupby(['station_id', 'date'])\n",
    "    .agg(\n",
    "        tavg=('tavg', 'mean'),\n",
    "        tmin=('tmin', 'mean'),\n",
    "        tmax=('tmax', 'mean'),\n",
    "        prcp_1d=('prcp_1d', 'sum'),  # daily precip (1-day)\n",
    "        wspd=('wspd', 'mean'),\n",
    "        pres=('pres', 'mean'),\n",
    "        STATE=('STATE', 'first'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "weather_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc553621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.3 Add lagged weather features (per station) ----\n",
    "def add_lagged_features(\n",
    "    df,\n",
    "    group_col,\n",
    "    date_col,\n",
    "    base_cols,\n",
    "    windows=(3, 7, 30),\n",
    "    shift=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each base_col, compute rolling means or sums over given windows,\n",
    "    then shift by `shift` days to avoid using same-day info.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([group_col, date_col]).copy()\n",
    "\n",
    "    for col in base_cols:\n",
    "        for w in windows:\n",
    "            roll = (\n",
    "                df.groupby(group_col)[col]\n",
    "                .transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
    "            )\n",
    "            df[f'{col}_mean_{w}'] = roll.shift(shift)\n",
    "\n",
    "    # Cumulative precipitation over 7 and 30 days, based on 1-day precip\n",
    "    for w in [7, 30]:\n",
    "        roll = (\n",
    "            df.groupby(group_col)['prcp_1d']\n",
    "            .transform(lambda x: x.rolling(w, min_periods=1).sum())\n",
    "        )\n",
    "        df[f'prcp_1d_sum_{w}'] = roll.shift(shift)\n",
    "\n",
    "    return df\n",
    "\n",
    "weather_feat = weather_daily.copy()\n",
    "\n",
    "# Impute any remaining NaNs before building lags (per station)\n",
    "weather_feat = weather_feat.set_index('station_id')\n",
    "weather_feat = weather_feat.groupby(level=0).ffill().bfill()\n",
    "weather_feat = weather_feat.reset_index()\n",
    "\n",
    "weather_feat['year'] = weather_feat['date'].dt.year\n",
    "weather_feat['month'] = weather_feat['date'].dt.month\n",
    "weather_feat['doy'] = weather_feat['date'].dt.dayofyear\n",
    "\n",
    "base_cols = ['tavg', 'tmin', 'tmax', 'prcp_1d', 'wspd', 'pres']\n",
    "\n",
    "weather_feat = add_lagged_features(\n",
    "    weather_feat,\n",
    "    group_col='station_id',\n",
    "    date_col='date',\n",
    "    base_cols=base_cols,\n",
    ")\n",
    "\n",
    "# Drop first few rows where lags are NaN (from shift)\n",
    "weather_feat = weather_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "weather_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60054cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3.1 Merge weather and fires on station + date + calendar / holiday features ----\n",
    "data = weather_feat.merge(\n",
    "    fires_daily, on=['station_id', 'date'], how='left'\n",
    ")\n",
    "\n",
    "# Replace NaNs in fire-related columns with 0\n",
    "fire_cols = [c for c in data.columns if c.startswith('cause_')] + [\n",
    "    'n_fires', 'area_burned', 'any_fire',\n",
    "]\n",
    "for c in fire_cols:\n",
    "    if c in data.columns:\n",
    "        data[c] = data[c].fillna(0)\n",
    "\n",
    "# Transformed target for area\n",
    "data['log_area_burned'] = np.log1p(data['area_burned'])\n",
    "\n",
    "# Weekend indicator\n",
    "data['is_weekend'] = data['date'].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "# US holidays\n",
    "us_holidays = holidays.US()\n",
    "data['holiday_name'] = data['date'].dt.date.map(us_holidays.get)\n",
    "data['is_holiday'] = data['holiday_name'].notna().astype(int)\n",
    "data['is_july4'] = (data['holiday_name'] == 'Independence Day').astype(int)\n",
    "\n",
    "# Combined \"weekend or holiday\" flag (optional extra feature)\n",
    "data['is_weekend_or_holiday'] = (\n",
    "    (data['is_weekend'] == 1) | (data['is_holiday'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3.1b Build merged feature tables for each distance cutoff ----\n",
    "# We reuse `weather_feat` and merge with each `fires_daily_d`.\n",
    "\n",
    "merged_by_cutoff = {}\n",
    "\n",
    "for d, fires_daily_d in fires_daily_by_cutoff.items():\n",
    "    data_d = weather_feat.merge(\n",
    "        fires_daily_d, on=['station_id', 'date'], how='left'\n",
    "    )\n",
    "\n",
    "    # Fire-related columns: cause_*, n_fires, area_burned, any_fire\n",
    "    fire_cols_d = [c for c in data_d.columns if c.startswith('cause_')] + [\n",
    "        'n_fires', 'area_burned', 'any_fire',\n",
    "    ]\n",
    "    for c in fire_cols_d:\n",
    "        if c in data_d.columns:\n",
    "            data_d[c] = data_d[c].fillna(0)\n",
    "\n",
    "    data_d['log_area_burned'] = np.log1p(data_d.get('area_burned', 0))\n",
    "\n",
    "    data_d['is_weekend'] = data_d['date'].dt.weekday.isin([5, 6]).astype(int)\n",
    "    us_holidays = holidays.US()\n",
    "    data_d['holiday_name'] = data_d['date'].dt.date.map(us_holidays.get)\n",
    "    data_d['is_holiday'] = data_d['holiday_name'].notna().astype(int)\n",
    "    data_d['is_july4'] = (data_d['holiday_name'] == 'Independence Day').astype(int)\n",
    "    data_d['is_weekend_or_holiday'] = (\n",
    "        (data_d['is_weekend'] == 1) | (data_d['is_holiday'] == 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    merged_by_cutoff[d] = data_d\n",
    "    print(f\"Built merged weather+fire table for cutoff {d} km with {len(data_d):,} rows\")\n",
    "\n",
    "# For backward compatibility, keep `data` as the unpruned version (d=0)\n",
    "if 0 in merged_by_cutoff:\n",
    "    data = merged_by_cutoff[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4.1 Encode STATE and station_id ----\n",
    "le_state = LabelEncoder()\n",
    "data['STATE_LE'] = le_state.fit_transform(\n",
    "    data['STATE'].fillna('UNK').astype(str)\n",
    " )\n",
    "\n",
    "le_station = LabelEncoder()\n",
    "data['station_LE'] = le_station.fit_transform(\n",
    "    data['station_id'].astype(str)\n",
    " )\n",
    "\n",
    "# ---- 4.2 Final feature list (excluding targets) ----\n",
    "# Conservative pruning of highly correlated helpers:\n",
    "pruned_cols = {\n",
    "    'doy',              # keep month as the main seasonal indicator\n",
    "    'prcp_1d_mean_7',   # keep prcp_1d_sum_7\n",
    "    'prcp_1d_mean_30',  # keep prcp_1d_sum_30\n",
    "}\n",
    "\n",
    "exclude_cols = {\n",
    "    'n_fires',\n",
    "    'any_fire',\n",
    "    'area_burned',\n",
    "    'log_area_burned',\n",
    "    'STATE',\n",
    "    'station_id',\n",
    "    'date',\n",
    "    'holiday_name',  # drop string column so all features are numeric\n",
    "} | set([c for c in data.columns if c.startswith('cause_')]) | pruned_cols\n",
    "\n",
    "feature_cols = [c for c in data.columns if c not in exclude_cols]\n",
    "len(feature_cols), feature_cols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4.3 Build feature/target sets per cutoff ----\n",
    "# We reuse the same `feature_cols` definition for all pruned datasets.\n",
    "\n",
    "features_by_cutoff = {}\n",
    "targets_by_cutoff = {}\n",
    "\n",
    "for d, data_d in merged_by_cutoff.items():\n",
    "    # Encode STATE and station_id using the same encoders as the base data\n",
    "    data_d = data_d.copy()\n",
    "    data_d['STATE_LE'] = le_state.transform(\n",
    "        data_d['STATE'].fillna('UNK').astype(str)\n",
    "    )\n",
    "    data_d['station_LE'] = le_station.transform(\n",
    "        data_d['station_id'].astype(str)\n",
    "    )\n",
    "\n",
    "    X_d = data_d[feature_cols]\n",
    "    y_count_d = data_d['n_fires']\n",
    "    y_bin_d = data_d['any_fire']\n",
    "    y_log_area_d = data_d['log_area_burned']\n",
    "\n",
    "    features_by_cutoff[d] = X_d\n",
    "    targets_by_cutoff[d] = {\n",
    "        'n_fires': y_count_d,\n",
    "        'any_fire': y_bin_d,\n",
    "        'log_area_burned': y_log_area_d,\n",
    "        'year': data_d['year'],\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Prepared feature/target matrices for cutoff {d} km: X shape = {X_d.shape}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ca8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the feature columns and drop any remaining NaNs\n",
    "corr_features = data[feature_cols].copy().dropna()\n",
    "\n",
    "# Compute Pearson correlation\n",
    "corr_matrix = corr_features.corr()\n",
    "\n",
    "# Quick summary: show the top 20 most correlated pairs (by absolute value, excluding self)\n",
    "corr_unstacked = (\n",
    "    corr_matrix.abs()\n",
    "    .where(~np.eye(corr_matrix.shape[0], dtype=bool))  # mask diagonal\n",
    "    .unstack()\n",
    "    .dropna()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top 20 strongest absolute correlations among features:\")\n",
    "print(corr_unstacked.head(20))\n",
    "\n",
    "# Optional: visualize a smaller subset (e.g., the most important features)\n",
    "top_feats = feature_importance_area.sort_values(\n",
    "    \"importance\", ascending=False\n",
    ")[\"feature\"].head(20).tolist() if \"feature_importance_area\" in globals() else feature_cols[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix.loc[top_feats, top_feats],\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.7},\n",
    ")\n",
    "plt.title(\"Correlation heatmap for top features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b36b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- X.2 Correlation among weather lag features ----\n",
    "lag_cols = [c for c in feature_cols if any(\n",
    "    key in c for key in [\"tavg_mean\", \"tmin_mean\", \"tmax_mean\", \"prcp_1d_sum\", \"wspd_mean\", \"pres_mean\"]\n",
    ")]\n",
    "\n",
    "corr_lags = data[lag_cols].copy().dropna().corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_lags,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.7},\n",
    ")\n",
    "plt.title(\"Correlation heatmap for lagged weather features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab8da6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe87e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5.1 Train / test split by year ----\n",
    "train_year_cutoff = 2011\n",
    "train_mask = data['year'] < train_year_cutoff\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_train = data.loc[train_mask, feature_cols]\n",
    "X_test = data.loc[test_mask, feature_cols]\n",
    "\n",
    "y_train_count = data.loc[train_mask, 'n_fires']\n",
    "y_test_count = data.loc[test_mask, 'n_fires']\n",
    "\n",
    "y_train_bin = data.loc[train_mask, 'any_fire']\n",
    "y_test_bin = data.loc[test_mask, 'any_fire']\n",
    "\n",
    "y_train_log_area = data.loc[train_mask, 'log_area_burned']\n",
    "y_test_log_area = data.loc[test_mask, 'log_area_burned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5.2 Train models for each distance cutoff ----\n",
    "# We will reuse the same LightGBM hyperparameters and year-based split.\n",
    "\n",
    "results_by_cutoff = {}\n",
    "\n",
    "for d in sorted(features_by_cutoff.keys()):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Training models for distance cutoff: {d} km\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    X_d = features_by_cutoff[d]\n",
    "    tgt = targets_by_cutoff[d]\n",
    "\n",
    "    year_d = tgt['year']\n",
    "    train_mask_d = year_d < train_year_cutoff\n",
    "    test_mask_d = ~train_mask_d\n",
    "\n",
    "    X_train_d = X_d.loc[train_mask_d]\n",
    "    X_test_d = X_d.loc[test_mask_d]\n",
    "\n",
    "    y_train_count_d = tgt['n_fires'].loc[train_mask_d]\n",
    "    y_test_count_d = tgt['n_fires'].loc[test_mask_d]\n",
    "\n",
    "    y_train_bin_d = tgt['any_fire'].loc[train_mask_d]\n",
    "    y_test_bin_d = tgt['any_fire'].loc[test_mask_d]\n",
    "\n",
    "    y_train_log_area_d = tgt['log_area_burned'].loc[train_mask_d]\n",
    "    y_test_log_area_d = tgt['log_area_burned'].loc[test_mask_d]\n",
    "\n",
    "    # Count model\n",
    "    lgb_train_d = lgb.Dataset(X_train_d, label=y_train_count_d)\n",
    "    lgb_valid_d = lgb.Dataset(X_test_d, label=y_test_count_d)\n",
    "\n",
    "    params_count = {\n",
    "    'objective': 'poisson',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "\n",
    "    model_count_d = lgb.train(\n",
    "        params_count,\n",
    "        lgb_train_d,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[lgb_valid_d],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=200),\n",
    "        ],\n",
    "    )\n",
    "    pred_count_d = model_count_d.predict(\n",
    "        X_test_d, num_iteration=model_count_d.best_iteration\n",
    "    )\n",
    "    rmse_count_d = np.sqrt(mean_squared_error(y_test_count_d, pred_count_d))\n",
    "\n",
    "    # Binary any-fire model\n",
    "    lgb_train_bin_d = lgb.Dataset(X_train_d, label=y_train_bin_d)\n",
    "    lgb_valid_bin_d = lgb.Dataset(X_test_d, label=y_test_bin_d)\n",
    "\n",
    "    params_bin = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"metric\": \"auc\",\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "    model_bin_d = lgb.train(\n",
    "        params_bin,\n",
    "        lgb_train_bin_d,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[lgb_valid_bin_d],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=200),\n",
    "        ],\n",
    "    )\n",
    "    prob_any_fire_d = model_bin_d.predict(\n",
    "        X_test_d, num_iteration=model_bin_d.best_iteration\n",
    "    )\n",
    "    auc_any_fire_d = roc_auc_score(y_test_bin_d, prob_any_fire_d)\n",
    "\n",
    "    # Area model\n",
    "    lgb_train_area_d = lgb.Dataset(X_train_d, label=y_train_log_area_d)\n",
    "    lgb_valid_area_d = lgb.Dataset(X_test_d, label=y_test_log_area_d)\n",
    "\n",
    "    params_area = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"metric\": \"rmse\",\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "    model_area_d = lgb.train(\n",
    "        params_area,\n",
    "        lgb_train_area_d,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[lgb_valid_area_d],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=200),\n",
    "        ],\n",
    "    )\n",
    "    pred_log_area_d = model_area_d.predict(\n",
    "        X_test_d, num_iteration=model_area_d.best_iteration\n",
    "    )\n",
    "    rmse_log_area_d = np.sqrt(\n",
    "        mean_squared_error(y_test_log_area_d, pred_log_area_d)\n",
    "    )\n",
    "\n",
    "    results_by_cutoff[d] = {\n",
    "        'rmse_count': rmse_count_d,\n",
    "        'auc_any_fire': auc_any_fire_d,\n",
    "        'rmse_log_area': rmse_log_area_d,\n",
    "    }\n",
    "\n",
    "    print(f\"Cutoff {d} km -> Count RMSE: {rmse_count_d:.3f}\")\n",
    "    print(f\"Cutoff {d} km -> AUC(any_fire): {auc_any_fire_d:.3f}\")\n",
    "    print(f\"Cutoff {d} km -> RMSE(log1p area): {rmse_log_area_d:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5.3 Summary of results ----\n",
    "results_summary = pd.DataFrame.from_dict(\n",
    "    results_by_cutoff, orient='index'\n",
    ")\n",
    "results_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740669d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results summary\n",
    "print(\"\\nSummary of results by distance cutoff (km):\")\n",
    "print(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sanity check: ensure no target columns are in feature_cols\n",
    "bad_targets = {'n_fires', 'any_fire', 'area_burned', 'log_area_burned'}\n",
    "bad_in_features = bad_targets.intersection(feature_cols)\n",
    "print('Targets in feature_cols:', bad_in_features)\n",
    " \n",
    "cause_in_features = [c for c in feature_cols if c.startswith('cause_')]\n",
    "print('Cause columns in feature_cols (should be empty):', cause_in_features)\n",
    "\n",
    "\n",
    "\n",
    "# Inspect distribution of predicted probabilities by label for one cutoff (e.g. 0 km)\n",
    "d = 0  # change to 100, 200, 400 to inspect others\n",
    "\n",
    "X_d = features_by_cutoff[d]\n",
    "tgt = targets_by_cutoff[d]\n",
    "year_d = tgt['year']\n",
    "train_mask_d = year_d < train_year_cutoff\n",
    "test_mask_d = ~train_mask_d\n",
    "\n",
    "X_test_d = X_d.loc[test_mask_d]\n",
    "y_test_bin_d = tgt['any_fire'].loc[test_mask_d]\n",
    "\n",
    "# Recompute the binary model for this cutoff only, reusing params_bin\n",
    "lgb_train_bin_d = lgb.Dataset(X_d.loc[train_mask_d], label=tgt['any_fire'].loc[train_mask_d])\n",
    "lgb_valid_bin_d = lgb.Dataset(X_test_d, label=y_test_bin_d)\n",
    "model_bin_d = lgb.train(\n",
    "    params_bin,\n",
    "    lgb_train_bin_d,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid_bin_d],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=200)],\n",
    ")\n",
    "prob_any_fire_d = model_bin_d.predict(X_test_d, num_iteration=model_bin_d.best_iteration)\n",
    "\n",
    "print('AUC for cutoff', d, ':', roc_auc_score(y_test_bin_d, prob_any_fire_d))\n",
    "\n",
    "print('\\nProbability summary for negative days (any_fire=0):')\n",
    "print(pd.Series(prob_any_fire_d[y_test_bin_d == 0]).describe())\n",
    "\n",
    "print('\\nProbability summary for positive days (any_fire=1):')\n",
    "print(pd.Series(prob_any_fire_d[y_test_bin_d == 1]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27caaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show pie of days with any_fire=1 vs any_fire=0\n",
    "labels = ['No Fire', 'Fire']\n",
    "sizes = [len(y_test_bin_d) - y_test_bin_d.sum(), y_test_bin_d.sum()]\n",
    "colors = ['lightblue', 'salmon']\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Any Fire Days in Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942add09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 6.1 Count model (Poisson) ----\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train_count)\n",
    "lgb_valid = lgb.Dataset(X_test, label=y_test_count)\n",
    "\n",
    "params_count = {\n",
    "    'objective': 'poisson',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "model_count = lgb.train(\n",
    "    params_count,\n",
    "    lgb_train,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
    ")\n",
    "\n",
    "pred_count = model_count.predict(X_test, num_iteration=model_count.best_iteration)\n",
    "rmse_count = np.sqrt(mean_squared_error(y_test_count, pred_count))\n",
    "\n",
    "print(f'Count model RMSE: {rmse_count:.3f}')\n",
    "print(f'Mean actual count in test: {y_test_count.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6914ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 6.2 Binary any-fire model (for AUC) ----\n",
    "params_bin = {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "lgb_train_bin = lgb.Dataset(X_train, label=y_train_bin)\n",
    "lgb_valid_bin = lgb.Dataset(X_test, label=y_test_bin)\n",
    "\n",
    "model_bin = lgb.train(\n",
    "    params_bin,\n",
    "    lgb_train_bin,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid_bin],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
    ")\n",
    "\n",
    "prob_any_fire = model_bin.predict(\n",
    "    X_test, num_iteration=model_bin.best_iteration\n",
    ")\n",
    "auc_any_fire = roc_auc_score(y_test_bin, prob_any_fire)\n",
    "\n",
    "print(f'AUC (any_fire): {auc_any_fire:.3f}')\n",
    "print(f'Positive rate (fires on a day): {y_test_bin.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 7.1 Area model ----\n",
    "params_area = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "lgb_train_area = lgb.Dataset(X_train, label=y_train_log_area)\n",
    "lgb_valid_area = lgb.Dataset(X_test, label=y_test_log_area)\n",
    "\n",
    "model_area = lgb.train(\n",
    "    params_area,\n",
    "    lgb_train_area,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid_area],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
    ")\n",
    "\n",
    "pred_log_area = model_area.predict(\n",
    "    X_test, num_iteration=model_area.best_iteration\n",
    ")\n",
    "\n",
    "rmse_log_area = np.sqrt(mean_squared_error(\n",
    "    y_test_log_area, pred_log_area\n",
    "))\n",
    "r2_log_area = r2_score(y_test_log_area, pred_log_area)\n",
    "\n",
    "print(f'Area model RMSE (log1p): {rmse_log_area:.3f}')\n",
    "print(f'Area model R^2 (log1p): {r2_log_area:.3f}')\n",
    "\n",
    "# Baseline: predict mean log-area of train\n",
    "baseline_log = np.full_like(y_test_log_area, y_train_log_area.mean())\n",
    "baseline_rmse_log = np.sqrt(mean_squared_error(\n",
    "    y_test_log_area, baseline_log\n",
    "))\n",
    "print(f'Baseline RMSE (log1p mean): {baseline_rmse_log:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 8.1 Prepare cause targets ----\n",
    "cause_cols = [c for c in data.columns if c.startswith('cause_')]\n",
    "len(cause_cols), cause_cols\n",
    "\n",
    "# Keep only days with at least one fire\n",
    "data_cause = data[data['n_fires'] > 0].copy()\n",
    "\n",
    "# Proportions per cause\n",
    "for c in cause_cols:\n",
    "    data_cause[c + '_prop'] = data_cause[c] / data_cause['n_fires']\n",
    "\n",
    "target_prop_cols = [c + '_prop' for c in cause_cols]\n",
    "\n",
    "# Train/test split by year (same cutoff)\n",
    "train_mask_cause = data_cause['year'] < train_year_cutoff\n",
    "test_mask_cause = ~train_mask_cause\n",
    "\n",
    "X_train_cause = data_cause.loc[train_mask_cause, feature_cols]\n",
    "X_test_cause = data_cause.loc[test_mask_cause, feature_cols]\n",
    "\n",
    "Y_train_cause = data_cause.loc[train_mask_cause, target_prop_cols].values\n",
    "Y_test_cause = data_cause.loc[test_mask_cause, target_prop_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed99cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 8.2 Multi-output regression for cause proportions ----\n",
    "base_reg = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    n_estimators=800,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "multi_reg = MultiOutputRegressor(base_reg)\n",
    "multi_reg.fit(X_train_cause, Y_train_cause)\n",
    "\n",
    "Y_pred_cause = multi_reg.predict(X_test_cause)\n",
    "\n",
    "# Ensure non-negative and normalize to sum to 1\n",
    "Y_pred_cause = np.clip(Y_pred_cause, 0, None)\n",
    "row_sums = Y_pred_cause.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1\n",
    "Y_pred_cause = Y_pred_cause / row_sums\n",
    "\n",
    "# ---- 8.3 Metrics: MSE of proportions and cross-entropy per fire ----\n",
    "mse_props = mean_squared_error(Y_test_cause.reshape(-1), Y_pred_cause.reshape(-1))\n",
    "print(f'MSE on cause proportions: {mse_props:.5f}')\n",
    "\n",
    "eps = 1e-12\n",
    "ce_per_day = -np.sum(Y_test_cause * np.log(Y_pred_cause + eps), axis=1)\n",
    "ce_mean = ce_per_day.mean()\n",
    "print(f'Mean cross-entropy (per day, per fire): {ce_mean:.3f}')\n",
    "\n",
    "# Baseline: always predict global average cause distribution (from train)\n",
    "global_dist = Y_train_cause.sum(axis=0)\n",
    "global_dist = global_dist / global_dist.sum()\n",
    "\n",
    "baseline_pred = np.tile(global_dist, (Y_test_cause.shape[0], 1))\n",
    "ce_baseline = -np.sum(Y_test_cause * np.log(baseline_pred + eps), axis=1).mean()\n",
    "mse_baseline = mean_squared_error(\n",
    "    Y_test_cause.reshape(-1), baseline_pred.reshape(-1)\n",
    ")\n",
    "\n",
    "print(f'Baseline CE: {ce_baseline:.3f}')\n",
    "print(f'Baseline MSE: {mse_baseline:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a446f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 9.1 Predicted vs Actual fire counts ----\n",
    "# Assumes y_test_count and pred_count are available from earlier cells\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test_count, y=pred_count, s=5, alpha=0.4)\n",
    "max_val = max(y_test_count.max(), pred_count.max())\n",
    "plt.plot([0, max_val], [0, max_val], color='red', linestyle='--', label='Ideal (y = x)')\n",
    "plt.xlabel('Actual fire count')\n",
    "plt.ylabel('Predicted fire count')\n",
    "plt.title('Predicted vs Actual Daily Fire Counts')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 9.2 Predicted vs Actual log area burned ----\n",
    "# Assumes y_test_log_area and pred_log_area are available\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test_log_area, y=pred_log_area, s=5, alpha=0.4)\n",
    "min_val = min(y_test_log_area.min(), pred_log_area.min())\n",
    "max_val = max(y_test_log_area.max(), pred_log_area.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Ideal (y = x)')\n",
    "plt.xlabel('Actual log1p(area_burned)')\n",
    "plt.ylabel('Predicted log1p(area_burned)')\n",
    "plt.title('Predicted vs Actual Log Area Burned')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 9.3 Feature importance for area model ----\n",
    "# Use LightGBM's built-in feature importance for model_area\n",
    "importances_area = model_area.feature_importance(importance_type='gain')\n",
    "feature_importance_area = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances_area,\n",
    "})\n",
    "feature_importance_area = feature_importance_area.sort_values('importance', ascending=False)\n",
    "top_n = 15\n",
    "top_features_area = feature_importance_area.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=top_features_area, x='importance', y='feature', orient='h')\n",
    "plt.title(f'Top {top_n} Features for Area Model (gain importance)')\n",
    "plt.xlabel('Importance (gain)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 9.4 Feature importance for specific causes (cause_1_prop and cause_4_prop) ----\n",
    "# Extract individual regressors from multi_reg\n",
    "# Assumes target_prop_cols corresponds to order of outputs in multi_reg\n",
    "cause_names = ['cause_1_prop', 'cause_4_prop']\n",
    "for cause_name in cause_names:\n",
    "    if cause_name not in target_prop_cols:\n",
    "        print(f\"Warning: {cause_name} not found in target_prop_cols; skipping.\")\n",
    "        continue\n",
    "\n",
    "    idx = target_prop_cols.index(cause_name)\n",
    "    reg = multi_reg.estimators_[idx]  # lightgbm.LGBMRegressor instance\n",
    "\n",
    "    fi = reg.feature_importances_\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': fi,\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    top_k = 10\n",
    "    top_fi = fi_df.head(top_k)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=top_fi, x='importance', y='feature', orient='h')\n",
    "    plt.title(f'Top {top_k} Features for {cause_name}')\n",
    "    plt.xlabel('Importance (split count)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---- 9.5 Distributions of log area burned ----\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y_train_log_area, bins=50, kde=True)\n",
    "plt.title('Train log1p(area_burned)')\n",
    "plt.xlabel('log1p(area_burned)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_test_log_area, bins=50, kde=True)\n",
    "plt.title('Test log1p(area_burned)')\n",
    "plt.xlabel('log1p(area_burned)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Train log-area:')\n",
    "print(y_train_log_area.describe())\n",
    "print('\\nTest log-area:')\n",
    "print(y_test_log_area.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 9.6 Visualize fitness vs. distance cutoff ----\n",
    "# Build a compact DataFrame of the summary metrics and plot vs cutoff.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "metrics_rows = []\n",
    "for d, res in results_by_cutoff.items():\n",
    "    metrics_rows.append({\n",
    "        'cutoff_km': d,\n",
    "        'rmse_count': res['rmse_count'],\n",
    "        'auc_any_fire': res['auc_any_fire'],\n",
    "        'rmse_log_area': res['rmse_log_area'],\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values('cutoff_km')\n",
    "print(metrics_df)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=metrics_df, x='cutoff_km', y='rmse_count', marker='o')\n",
    "plt.title('Count RMSE vs. distance cutoff')\n",
    "plt.xlabel('Minimum fire–station distance kept (km)')\n",
    "plt.ylabel('RMSE (fire count)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=metrics_df, x='cutoff_km', y='auc_any_fire', marker='o')\n",
    "plt.title('Any-fire AUC vs. distance cutoff')\n",
    "plt.xlabel('Minimum fire–station distance kept (km)')\n",
    "plt.ylabel('AUC (any_fire)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.lineplot(data=metrics_df, x='cutoff_km', y='rmse_log_area', marker='o')\n",
    "plt.title('Log-area RMSE vs. distance cutoff')\n",
    "plt.xlabel('Minimum fire–station distance kept (km)')\n",
    "plt.ylabel('RMSE (log1p area)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360706b1",
   "metadata": {},
   "source": [
    "# Feature and target glossary\n",
    "\n",
    "\n",
    "This section summarizes the main engineered features and targets used in this notebook.\n",
    "\n",
    "\n",
    "\n",
    "## Identifiers and time fields\n",
    "\n",
    "\n",
    "- `station_id`: ID of the nearest weather station associated with each fire and each daily weather record.\n",
    "\n",
    "- `STATE`: Two-letter US state abbreviation for the station.\n",
    "\n",
    "- `date`: Calendar date (day-level granularity) for each station-day.\n",
    "\n",
    "- `year`, `month`, `doy`: Calendar year, month (1–12), and day-of-year (1–366) derived from `date`.\n",
    "\n",
    "\n",
    "\n",
    "## Weather features (daily aggregates)\n",
    "\n",
    "\n",
    "\n",
    "Base daily weather variables aggregated per station and date:\n",
    "\n",
    "\n",
    "\n",
    "- `tavg`: Mean daily temperature (°C).\n",
    "\n",
    "- `tmin`: Minimum daily temperature (°C).\n",
    "\n",
    "- `tmax`: Maximum daily temperature (°C).\n",
    "\n",
    "- `prcp_1d`: Total daily precipitation (mm) for that day.\n",
    "\n",
    "- `wspd`: Average daily wind speed (km/h).\n",
    "\n",
    "- `pres`: Average daily sea-level pressure (hPa).\n",
    "\n",
    "\n",
    "\n",
    "## Lagged and windowed weather features\n",
    "\n",
    "\n",
    "\n",
    "To avoid look-ahead bias, rolling features are computed within each station, then shifted by 1 day so only **past** weather is used to predict fires on a given day.\n",
    "\n",
    "\n",
    "\n",
    "For each base column in `[tavg, tmin, tmax, prcp_1d, wspd, pres]` and for window sizes 3, 7, and 30 days:\n",
    "\n",
    "\n",
    "\n",
    "- `<col>_mean_3`, `<col>_mean_7`, `<col>_mean_30`  \n",
    "\n",
    "  Rolling mean of the last 3, 7, or 30 days of that variable, shifted by 1 day.\n",
    "\n",
    "\n",
    "\n",
    "For precipitation specifically:\n",
    "\n",
    "\n",
    "\n",
    "- `prcp_1d_sum_7`, `prcp_1d_sum_30`  \n",
    "\n",
    "  Rolling sum of daily precipitation over the last 7 or 30 days (also shifted by 1 day).\n",
    "\n",
    "\n",
    "\n",
    "These lags capture short- and medium-term weather patterns leading up to potential fires.\n",
    "\n",
    "\n",
    "\n",
    "## Calendar and holiday features\n",
    "\n",
    "\n",
    "\n",
    "- `is_weekend`: 1 if `date` is Saturday or Sunday, otherwise 0.\n",
    "\n",
    "- `is_holiday`: 1 if the date is a US federal holiday (from the `holidays` library), otherwise 0.\n",
    "\n",
    "- `is_july4`: 1 if the date is **Independence Day**, otherwise 0.  \n",
    "\n",
    "  This is a proxy for increased fire risk driven by fireworks.\n",
    "\n",
    "- `is_weekend_or_holiday`: 1 if either `is_weekend` or `is_holiday` is 1, otherwise 0.\n",
    "\n",
    "\n",
    "\n",
    "These features capture human behavior patterns (weekends, holidays, July 4) that can influence ignition risk.\n",
    "\n",
    "\n",
    "\n",
    "## Encoded identifiers\n",
    "\n",
    "\n",
    "\n",
    "- `STATE_LE`: Integer label encoding of `STATE` (state-level categorical information).\n",
    "\n",
    "- `station_LE`: Integer label encoding of `station_id` (station identity).\n",
    "\n",
    "\n",
    "\n",
    "These are used as numeric proxies for location in the models.\n",
    "\n",
    "\n",
    "\n",
    "## Fire activity targets and cause counts\n",
    "\n",
    "\n",
    "\n",
    "Station-day fire aggregates:\n",
    "\n",
    "\n",
    "\n",
    "- `n_fires`: Number of fires discovered on that station-day.\n",
    "\n",
    "- `area_burned`: Sum of `FIRE_SIZE` (area burned) across all fires on that station-day.\n",
    "\n",
    "- `any_fire`: Binary indicator equal to 1 if at least one fire occurred (`n_fires > 0`), 0 otherwise.\n",
    "\n",
    "\n",
    "\n",
    "Cause-specific daily counts (from pivoting `STAT_CAUSE_CODE`):\n",
    "\n",
    "\n",
    "\n",
    "- `cause_1`, `cause_2`, ..., `cause_k`: Count of fires on that day attributed to each cause code `k`.\n",
    "\n",
    "\n",
    "\n",
    "For the cause-proportion model (only on days with fires):\n",
    "\n",
    "\n",
    "\n",
    "- `cause_<k>_prop`: Proportion of fires on that day attributed to cause code `k`, i.e.  \n",
    "\n",
    "  `cause_<k>_prop = cause_<k> / n_fires`.\n",
    "\n",
    "\n",
    "\n",
    "These per-cause proportions are used as multi-output regression targets in the `multi_reg` LightGBM model.\n",
    "\n",
    "\n",
    "\n",
    "## Transformed area target\n",
    "\n",
    "\n",
    "\n",
    "- `log_area_burned`: `log1p(area_burned)`  \n",
    "\n",
    "  A log-transform of total burned area to stabilize variance and reduce the impact of very large fires.  \n",
    "\n",
    "  This is the target for the **area model** (`model_area`).\n",
    "\n",
    "\n",
    "\n",
    "### Train/test targets used in models\n",
    "\n",
    "\n",
    "\n",
    "- `y_train_count`, `y_test_count`: Training and test labels for the **count model** (predicting `n_fires`).\n",
    "\n",
    "- `y_train_bin`, `y_test_bin`: Training and test labels for the **binary any-fire model** (predicting `any_fire`).\n",
    "\n",
    "- `y_train_log_area`, `y_test_log_area`: Training and test labels for the **area model** (predicting `log_area_burned`).\n",
    "\n",
    "\n",
    "\n",
    "Together, these features and targets allow the notebook to model:\n",
    "\n",
    "\n",
    "\n",
    "1. How many fires occur on a given station-day.\n",
    "\n",
    "2. Whether any fire occurs at all.\n",
    "\n",
    "3. How much area burns (on a log scale).\n",
    "\n",
    "4. How the mix of ignition causes varies with weather, calendar, and location.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
