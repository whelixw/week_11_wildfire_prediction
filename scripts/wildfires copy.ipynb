{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce95e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "import holidays\n",
    "\n",
    "DATA_DIR = Path('../shared_data/')  # change to your local path\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ab33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.1 Load wildfires ----\n",
    "fires_path = DATA_DIR / 'wildfires_fires.parquet'\n",
    "\n",
    "fires = pd.read_parquet(\n",
    "    fires_path,\n",
    "    columns=[\n",
    "        'FIRE_SIZE',\n",
    "        'DISCOVERY_DATE',\n",
    "        'STATE',\n",
    "        'STAT_CAUSE_CODE',\n",
    "        'LATITUDE',\n",
    "        'LONGITUDE',\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Convert DISCOVERY_DATE to datetime\n",
    "if not np.issubdtype(fires['DISCOVERY_DATE'].dtype, np.datetime64):\n",
    "    fires['DISCOVERY_DATE'] = pd.to_datetime(fires['DISCOVERY_DATE'])\n",
    "\n",
    "fires = fires.rename(columns={'DISCOVERY_DATE': 'date'})\n",
    "\n",
    "# Filter date range 1992–2015\n",
    "fires = fires[\n",
    "    (fires['date'].dt.year >= 1992) & (fires['date'].dt.year <= 2015)\n",
    "]\n",
    "\n",
    "# Drop rows without coordinates\n",
    "fires = fires.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b963d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.1b Attach closest weather station to each fire ----\n",
    "cities_path = DATA_DIR / 'cities.csv'\n",
    "stations = pd.read_csv(cities_path)\n",
    "\n",
    "# Keep one row per station with coordinates\n",
    "stations = stations[[\"station_id\", \"latitude\", \"longitude\"]].drop_duplicates('station_id')\n",
    "\n",
    "# Build BallTree (coordinates in radians)\n",
    "station_coords = np.radians(stations[['latitude', 'longitude']].values)\n",
    "fire_coords = np.radians(fires[['LATITUDE', 'LONGITUDE']].values)\n",
    "\n",
    "tree = BallTree(station_coords, metric='haversine')\n",
    "dist_rad, ind = tree.query(fire_coords, k=1)\n",
    "\n",
    "fires['station_id'] = stations.iloc[ind.flatten()]['station_id'].values\n",
    "fires['dist_to_station_km'] = dist_rad.flatten() * 6371.0  # earth radius ~6371 km\n",
    "\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55dfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1.2 Aggregate to station–day ----\n",
    "\n",
    "# Basic daily targets per station\n",
    "daily_agg = (\n",
    "    fires.groupby(['station_id', 'date'])\n",
    "    .agg(\n",
    "        n_fires=('FIRE_SIZE', 'size'),\n",
    "        area_burned=('FIRE_SIZE', 'sum'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Cause counts per station–day (wide format)\n",
    "cause_counts = (\n",
    "    fires.pivot_table(\n",
    "        index=['station_id', 'date'],\n",
    "        columns='STAT_CAUSE_CODE',\n",
    "        values='FIRE_SIZE',  # any column, we just count\n",
    "        aggfunc='count',\n",
    "        fill_value=0,\n",
    "    )\n",
    "    .rename_axis(columns='cause_code')\n",
    ")\n",
    "\n",
    "# Make nicer column names: cause_1, cause_2, ...\n",
    "cause_counts.columns = [f'cause_{int(c)}' for c in cause_counts.columns]\n",
    "cause_counts = cause_counts.reset_index()\n",
    "\n",
    "# Merge counts into daily_agg\n",
    "fires_daily = daily_agg.merge(\n",
    "    cause_counts, on=['station_id', 'date'], how='left'\n",
    ").fillna(0)\n",
    "\n",
    "fires_daily['any_fire'] = (fires_daily['n_fires'] > 0).astype(int)\n",
    "fires_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50460dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.1 Load weather (station-based) ----\n",
    "weather_path = DATA_DIR / 'us_daily_weather_1992_2015.parquet'\n",
    "weather = pd.read_parquet(weather_path)\n",
    "\n",
    "# Ensure date is datetime\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "\n",
    "# Restrict to 1992–2015\n",
    "weather = weather[\n",
    "    (weather['date'].dt.year >= 1992) & (weather['date'].dt.year <= 2015)\n",
    "]\n",
    "\n",
    "# Load station metadata (cities.csv)\n",
    "cities_path = DATA_DIR / 'cities.csv'\n",
    "cities_df = pd.read_csv(cities_path)\n",
    "\n",
    "# Merge state info into weather via station_id\n",
    "weather = weather.merge(\n",
    "    cities_df[['station_id', 'state']].drop_duplicates('station_id'),\n",
    "    on='station_id',\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "weather = weather.rename(\n",
    "    columns={\n",
    "        'state': 'STATE',\n",
    "        'avg_temp_c': 'tavg',\n",
    "        'min_temp_c': 'tmin',\n",
    "        'max_temp_c': 'tmax',\n",
    "        'precipitation_mm': 'prcp_1d',  # explicit 1-day precipitation\n",
    "        'avg_wind_speed_kmh': 'wspd',\n",
    "        'avg_sea_level_pres_hpa': 'pres',\n",
    "    }\n",
    ")\n",
    "\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.2 Aggregate station data to station–day ----\n",
    "weather_daily = (\n",
    "    weather.groupby(['station_id', 'date'])\n",
    "    .agg(\n",
    "        tavg=('tavg', 'mean'),\n",
    "        tmin=('tmin', 'mean'),\n",
    "        tmax=('tmax', 'mean'),\n",
    "        prcp_1d=('prcp_1d', 'sum'),  # daily precip (1-day)\n",
    "        wspd=('wspd', 'mean'),\n",
    "        pres=('pres', 'mean'),\n",
    "        STATE=('STATE', 'first'),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "weather_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc553621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2.3 Add lagged weather features (per station) ----\n",
    "def add_lagged_features(\n",
    "    df,\n",
    "    group_col,\n",
    "    date_col,\n",
    "    base_cols,\n",
    "    windows=(3, 7, 30),\n",
    "    shift=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each base_col, compute rolling means or sums over given windows,\n",
    "    then shift by `shift` days to avoid using same-day info.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([group_col, date_col]).copy()\n",
    "\n",
    "    for col in base_cols:\n",
    "        for w in windows:\n",
    "            roll = (\n",
    "                df.groupby(group_col)[col]\n",
    "                .transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
    "            )\n",
    "            df[f'{col}_mean_{w}'] = roll.shift(shift)\n",
    "\n",
    "    # Cumulative precipitation over 7 and 30 days, based on 1-day precip\n",
    "    for w in [7, 30]:\n",
    "        roll = (\n",
    "            df.groupby(group_col)['prcp_1d']\n",
    "            .transform(lambda x: x.rolling(w, min_periods=1).sum())\n",
    "        )\n",
    "        df[f'prcp_1d_sum_{w}'] = roll.shift(shift)\n",
    "\n",
    "    return df\n",
    "\n",
    "weather_feat = weather_daily.copy()\n",
    "\n",
    "# Impute any remaining NaNs before building lags (per station)\n",
    "weather_feat = weather_feat.set_index('station_id')\n",
    "weather_feat = weather_feat.groupby(level=0).ffill().bfill()\n",
    "weather_feat = weather_feat.reset_index()\n",
    "\n",
    "weather_feat['year'] = weather_feat['date'].dt.year\n",
    "weather_feat['month'] = weather_feat['date'].dt.month\n",
    "weather_feat['doy'] = weather_feat['date'].dt.dayofyear\n",
    "\n",
    "base_cols = ['tavg', 'tmin', 'tmax', 'prcp_1d', 'wspd', 'pres']\n",
    "\n",
    "weather_feat = add_lagged_features(\n",
    "    weather_feat,\n",
    "    group_col='station_id',\n",
    "    date_col='date',\n",
    "    base_cols=base_cols,\n",
    ")\n",
    "\n",
    "# Drop first few rows where lags are NaN (from shift)\n",
    "weather_feat = weather_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "weather_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60054cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3.1 Merge weather and fires on station + date + calendar / holiday features ----\n",
    "data = weather_feat.merge(\n",
    "    fires_daily, on=['station_id', 'date'], how='left'\n",
    ")\n",
    "\n",
    "# Replace NaNs in fire-related columns with 0\n",
    "fire_cols = [c for c in data.columns if c.startswith('cause_')] + [\n",
    "    'n_fires', 'area_burned', 'any_fire',\n",
    "]\n",
    "for c in fire_cols:\n",
    "    if c in data.columns:\n",
    "        data[c] = data[c].fillna(0)\n",
    "\n",
    "# Transformed target for area\n",
    "data['log_area_burned'] = np.log1p(data['area_burned'])\n",
    "\n",
    "# Weekend indicator\n",
    "data['is_weekend'] = data['date'].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "# US holidays\n",
    "us_holidays = holidays.US()\n",
    "data['holiday_name'] = data['date'].dt.date.map(us_holidays.get)\n",
    "data['is_holiday'] = data['holiday_name'].notna().astype(int)\n",
    "data['is_july4'] = (data['holiday_name'] == 'Independence Day').astype(int)\n",
    "\n",
    "# Combined \"weekend or holiday\" flag (optional extra feature)\n",
    "data['is_weekend_or_holiday'] = (\n",
    "    (data['is_weekend'] == 1) | (data['is_holiday'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4.1 Encode STATE and station_id ----\n",
    "le_state = LabelEncoder()\n",
    "data['STATE_LE'] = le_state.fit_transform(\n",
    "    data['STATE'].fillna('UNK').astype(str)\n",
    " )\n",
    "\n",
    "le_station = LabelEncoder()\n",
    "data['station_LE'] = le_station.fit_transform(\n",
    "    data['station_id'].astype(str)\n",
    " )\n",
    "\n",
    "# ---- 4.2 Final feature list (excluding targets) ----\n",
    "exclude_cols = {\n",
    "    'n_fires',\n",
    "    'any_fire',\n",
    "    'area_burned',\n",
    "    'log_area_burned',\n",
    "    'STATE',\n",
    "    'station_id',\n",
    "    'date',\n",
    "    'holiday_name',  # drop string column so all features are numeric\n",
    "} | set([c for c in data.columns if c.startswith('cause_')])\n",
    "\n",
    "feature_cols = [c for c in data.columns if c not in exclude_cols]\n",
    "len(feature_cols), feature_cols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe87e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5.1 Train / test split by year ----\n",
    "train_year_cutoff = 2011\n",
    "train_mask = data['year'] < train_year_cutoff\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_train = data.loc[train_mask, feature_cols]\n",
    "X_test = data.loc[test_mask, feature_cols]\n",
    "\n",
    "y_train_count = data.loc[train_mask, 'n_fires']\n",
    "y_test_count = data.loc[test_mask, 'n_fires']\n",
    "\n",
    "y_train_bin = data.loc[train_mask, 'any_fire']\n",
    "y_test_bin = data.loc[test_mask, 'any_fire']\n",
    "\n",
    "y_train_log_area = data.loc[train_mask, 'log_area_burned']\n",
    "y_test_log_area = data.loc[test_mask, 'log_area_burned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942add09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 6.1 Count model (Poisson) ----\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train_count)\n",
    "lgb_valid = lgb.Dataset(X_test, label=y_test_count)\n",
    "\n",
    "params_count = {\n",
    "    'objective': 'poisson',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "model_count = lgb.train(\n",
    "    params_count,\n",
    "    lgb_train,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
    ")\n",
    "\n",
    "pred_count = model_count.predict(X_test, num_iteration=model_count.best_iteration)\n",
    "rmse_count = np.sqrt(mean_squared_error(y_test_count, pred_count))\n",
    "\n",
    "print(f'Count model RMSE: {rmse_count:.3f}')\n",
    "print(f'Mean actual count in test: {y_test_count.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6914ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 6.2 Binary any-fire model (for AUC) ----\n",
    "params_bin = {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "lgb_train_bin = lgb.Dataset(X_train, label=y_train_bin)\n",
    "lgb_valid_bin = lgb.Dataset(X_test, label=y_test_bin)\n",
    "\n",
    "model_bin = lgb.train(\n",
    "    params_bin,\n",
    "    lgb_train_bin,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid_bin],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
    ")\n",
    "\n",
    "prob_any_fire = model_bin.predict(\n",
    "    X_test, num_iteration=model_bin.best_iteration\n",
    ")\n",
    "auc_any_fire = roc_auc_score(y_test_bin, prob_any_fire)\n",
    "\n",
    "print(f'AUC (any_fire): {auc_any_fire:.3f}')\n",
    "print(f'Positive rate (fires on a day): {y_test_bin.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 7.1 Area model ----\n",
    "params_area = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "lgb_train_area = lgb.Dataset(X_train, label=y_train_log_area)\n",
    "lgb_valid_area = lgb.Dataset(X_test, label=y_test_log_area)\n",
    "\n",
    "model_area = lgb.train(\n",
    "    params_area,\n",
    "    lgb_train_area,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_valid_area],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],\n",
    ")\n",
    "\n",
    "pred_log_area = model_area.predict(\n",
    "    X_test, num_iteration=model_area.best_iteration\n",
    ")\n",
    "\n",
    "rmse_log_area = np.sqrt(mean_squared_error(\n",
    "    y_test_log_area, pred_log_area\n",
    "))\n",
    "r2_log_area = r2_score(y_test_log_area, pred_log_area)\n",
    "\n",
    "print(f'Area model RMSE (log1p): {rmse_log_area:.3f}')\n",
    "print(f'Area model R^2 (log1p): {r2_log_area:.3f}')\n",
    "\n",
    "# Baseline: predict mean log-area of train\n",
    "baseline_log = np.full_like(y_test_log_area, y_train_log_area.mean())\n",
    "baseline_rmse_log = np.sqrt(mean_squared_error(\n",
    "    y_test_log_area, baseline_log\n",
    "))\n",
    "print(f'Baseline RMSE (log1p mean): {baseline_rmse_log:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 8.1 Prepare cause targets ----\n",
    "cause_cols = [c for c in data.columns if c.startswith('cause_')]\n",
    "len(cause_cols), cause_cols\n",
    "\n",
    "# Keep only days with at least one fire\n",
    "data_cause = data[data['n_fires'] > 0].copy()\n",
    "\n",
    "# Proportions per cause\n",
    "for c in cause_cols:\n",
    "    data_cause[c + '_prop'] = data_cause[c] / data_cause['n_fires']\n",
    "\n",
    "target_prop_cols = [c + '_prop' for c in cause_cols]\n",
    "\n",
    "# Train/test split by year (same cutoff)\n",
    "train_mask_cause = data_cause['year'] < train_year_cutoff\n",
    "test_mask_cause = ~train_mask_cause\n",
    "\n",
    "X_train_cause = data_cause.loc[train_mask_cause, feature_cols]\n",
    "X_test_cause = data_cause.loc[test_mask_cause, feature_cols]\n",
    "\n",
    "Y_train_cause = data_cause.loc[train_mask_cause, target_prop_cols].values\n",
    "Y_test_cause = data_cause.loc[test_mask_cause, target_prop_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed99cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 8.2 Multi-output regression for cause proportions ----\n",
    "base_reg = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    n_estimators=800,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "multi_reg = MultiOutputRegressor(base_reg)\n",
    "multi_reg.fit(X_train_cause, Y_train_cause)\n",
    "\n",
    "Y_pred_cause = multi_reg.predict(X_test_cause)\n",
    "\n",
    "# Ensure non-negative and normalize to sum to 1\n",
    "Y_pred_cause = np.clip(Y_pred_cause, 0, None)\n",
    "row_sums = Y_pred_cause.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1\n",
    "Y_pred_cause = Y_pred_cause / row_sums\n",
    "\n",
    "# ---- 8.3 Metrics: MSE of proportions and cross-entropy per fire ----\n",
    "mse_props = mean_squared_error(Y_test_cause.reshape(-1), Y_pred_cause.reshape(-1))\n",
    "print(f'MSE on cause proportions: {mse_props:.5f}')\n",
    "\n",
    "eps = 1e-12\n",
    "ce_per_day = -np.sum(Y_test_cause * np.log(Y_pred_cause + eps), axis=1)\n",
    "ce_mean = ce_per_day.mean()\n",
    "print(f'Mean cross-entropy (per day, per fire): {ce_mean:.3f}')\n",
    "\n",
    "# Baseline: always predict global average cause distribution (from train)\n",
    "global_dist = Y_train_cause.sum(axis=0)\n",
    "global_dist = global_dist / global_dist.sum()\n",
    "\n",
    "baseline_pred = np.tile(global_dist, (Y_test_cause.shape[0], 1))\n",
    "ce_baseline = -np.sum(Y_test_cause * np.log(baseline_pred + eps), axis=1).mean()\n",
    "mse_baseline = mean_squared_error(\n",
    "    Y_test_cause.reshape(-1), baseline_pred.reshape(-1)\n",
    ")\n",
    "\n",
    "print(f'Baseline CE: {ce_baseline:.3f}')\n",
    "print(f'Baseline MSE: {mse_baseline:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a446f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 9.1 Predicted vs Actual fire counts ----\n",
    "# Assumes y_test_count and pred_count are available from earlier cells\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test_count, y=pred_count, s=5, alpha=0.4)\n",
    "max_val = max(y_test_count.max(), pred_count.max())\n",
    "plt.plot([0, max_val], [0, max_val], color='red', linestyle='--', label='Ideal (y = x)')\n",
    "plt.xlabel('Actual fire count')\n",
    "plt.ylabel('Predicted fire count')\n",
    "plt.title('Predicted vs Actual Daily Fire Counts')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 9.2 Predicted vs Actual log area burned ----\n",
    "# Assumes y_test_log_area and pred_log_area are available\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test_log_area, y=pred_log_area, s=5, alpha=0.4)\n",
    "min_val = min(y_test_log_area.min(), pred_log_area.min())\n",
    "max_val = max(y_test_log_area.max(), pred_log_area.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Ideal (y = x)')\n",
    "plt.xlabel('Actual log1p(area_burned)')\n",
    "plt.ylabel('Predicted log1p(area_burned)')\n",
    "plt.title('Predicted vs Actual Log Area Burned')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 9.3 Feature importance for area model ----\n",
    "# Use LightGBM's built-in feature importance for model_area\n",
    "importances_area = model_area.feature_importance(importance_type='gain')\n",
    "feature_importance_area = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances_area,\n",
    "})\n",
    "feature_importance_area = feature_importance_area.sort_values('importance', ascending=False)\n",
    "top_n = 15\n",
    "top_features_area = feature_importance_area.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=top_features_area, x='importance', y='feature', orient='h')\n",
    "plt.title(f'Top {top_n} Features for Area Model (gain importance)')\n",
    "plt.xlabel('Importance (gain)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 9.4 Feature importance for specific causes (cause_1_prop and cause_4_prop) ----\n",
    "# Extract individual regressors from multi_reg\n",
    "# Assumes target_prop_cols corresponds to order of outputs in multi_reg\n",
    "cause_names = ['cause_1_prop', 'cause_4_prop']\n",
    "for cause_name in cause_names:\n",
    "    if cause_name not in target_prop_cols:\n",
    "        print(f\"Warning: {cause_name} not found in target_prop_cols; skipping.\")\n",
    "        continue\n",
    "\n",
    "    idx = target_prop_cols.index(cause_name)\n",
    "    reg = multi_reg.estimators_[idx]  # lightgbm.LGBMRegressor instance\n",
    "\n",
    "    fi = reg.feature_importances_\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': fi,\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    top_k = 10\n",
    "    top_fi = fi_df.head(top_k)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=top_fi, x='importance', y='feature', orient='h')\n",
    "    plt.title(f'Top {top_k} Features for {cause_name}')\n",
    "    plt.xlabel('Importance (split count)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---- 9.5 Rename cause columns to descriptive names ----\n",
    "# Build mapping from STAT_CAUSE_CODE to STAT_CAUSE_DESCR from original fires dataset\n",
    "fires_for_mapping = fires[['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']].drop_duplicates()\n",
    "fires_for_mapping = fires_for_mapping.dropna(subset=['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR'])\n",
    "\n",
    "code_to_descr = (\n",
    "    fires_for_mapping.set_index('STAT_CAUSE_CODE')['STAT_CAUSE_DESCR'].to_dict()\n",
    " )\n",
    "\n",
    "def clean_descr(d):\n",
    "    d = d.lower().strip()\n",
    "    d = d.replace('&', 'and')\n",
    "    d = d.replace('/', ' ')\n",
    "    d = d.replace('-', ' ')\n",
    "    d = d.replace(',', ' ')\n",
    "    d = '_'.join(part for part in d.split() if part)\n",
    "    return d\n",
    "\n",
    "rename_map = {}\n",
    "for col in data.columns:\n",
    "    if col.startswith('cause_') and col.endswith('_prop') is False:\n",
    "        # raw count columns like cause_1, cause_2, ...\n",
    "        try:\n",
    "            code = int(col.split('_')[1])\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "        if code in code_to_descr:\n",
    "            descr = clean_descr(code_to_descr[code])\n",
    "            rename_map[col] = f'cause_{descr}'\n",
    "\n",
    "# Apply renaming to data and cause_cols / target_prop_cols if desired\n",
    "data = data.rename(columns=rename_map)\n",
    "cause_cols = [rename_map.get(c, c) for c in cause_cols]\n",
    "target_prop_cols = [c + '_prop' for c in cause_cols]\n",
    "\n",
    "# ---- 9.6 Distributions of log area burned ----\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y_train_log_area, bins=50, kde=True)\n",
    "plt.title('Train log1p(area_burned)')\n",
    "plt.xlabel('log1p(area_burned)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_test_log_area, bins=50, kde=True)\n",
    "plt.title('Test log1p(area_burned)')\n",
    "plt.xlabel('log1p(area_burned)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Train log-area:')\n",
    "print(y_train_log_area.describe())\n",
    "print('\\nTest log-area:')\n",
    "print(y_test_log_area.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
