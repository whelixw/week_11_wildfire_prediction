# -*- coding: utf-8 -*-
"""get_github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nfyl8b5OHMb_U4bJ9egWdjb_a__L_MTO
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone the entire repo.
!git clone -l -s https://github.com/whelixw/week_11_wildfire_prediction.git wildfire_prediction
# %cd wildfire_prediction/
!ls

import numpy as np
import pandas as pd
from pathlib import Path

import lightgbm as lgb
from lightgbm import LGBMRegressor

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.multioutput import MultiOutputRegressor
from sklearn.neighbors import BallTree
from sklearn.metrics import (
    mean_squared_error,
    roc_auc_score,
    r2_score,
)

import holidays

DATA_DIR = Path("shared_data/")  # change to your local path
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)


# ---- 1.1 Load wildfires ----
fires_path = DATA_DIR / "wildfires_fires.parquet"

fires = pd.read_parquet(
    fires_path,
    columns=[
        "FIRE_SIZE",
        "DISCOVERY_DATE",
        "STATE",
        "STAT_CAUSE_CODE",
        "LATITUDE",
        "LONGITUDE",
    ],
)

# Convert DISCOVERY_DATE to datetime
if not np.issubdtype(fires["DISCOVERY_DATE"].dtype, np.datetime64):
    fires["DISCOVERY_DATE"] = pd.to_datetime(fires["DISCOVERY_DATE"])

fires = fires.rename(columns={"DISCOVERY_DATE": "date"})

# Filter date range 1992–2015
fires = fires[
    (fires["date"].dt.year >= 1992) & (fires["date"].dt.year <= 2015)
]

# Drop rows without coordinates
fires = fires.dropna(subset=["LATITUDE", "LONGITUDE"])

# ---- 1.1b Attach closest weather station to each fire ----
cities_path = DATA_DIR / "cities.csv"
stations = pd.read_csv(cities_path)

# Keep one row per station with coordinates
stations = stations[
    ["station_id", "latitude", "longitude"]
].drop_duplicates("station_id")

# Build BallTree (coordinates in radians)
station_coords = np.radians(stations[["latitude", "longitude"]].values)
fire_coords = np.radians(fires[["LATITUDE", "LONGITUDE"]].values)

tree = BallTree(station_coords, metric="haversine")
dist_rad, ind = tree.query(fire_coords, k=1)

fires["station_id"] = stations.iloc[ind.flatten()]["station_id"].values
fires["dist_to_station_km"] = dist_rad.flatten() * 6371.0  # earth radius ~6371 km

fires.head()

# ---- 1.2 Aggregate to station–day ----

# Basic daily targets per station
daily_agg = (
    fires.groupby(["station_id", "date"])
    .agg(
        n_fires=("FIRE_SIZE", "size"),
        area_burned=("FIRE_SIZE", "sum"),
    )
    .reset_index()
)

# Cause counts per station–day (wide format)
cause_counts = (
    fires.pivot_table(
        index=["station_id", "date"],
        columns="STAT_CAUSE_CODE",
        values="FIRE_SIZE",  # any column, we just count
        aggfunc="count",
        fill_value=0,
    )
    .rename_axis(columns="cause_code")
)

# Make nicer column names: cause_1, cause_2, ...
cause_counts.columns = [f"cause_{int(c)}" for c in cause_counts.columns]
cause_counts = cause_counts.reset_index()

# Merge counts into daily_agg
fires_daily = daily_agg.merge(
    cause_counts, on=["station_id", "date"], how="left"
).fillna(0)

fires_daily["any_fire"] = (fires_daily["n_fires"] > 0).astype(int)

fires_daily.head()

# ---- 2.1 Load weather (station-based) ----
weather_path = DATA_DIR / "us_daily_weather_1992_2015.parquet"
weather = pd.read_parquet(weather_path)

# Ensure date is datetime
weather["date"] = pd.to_datetime(weather["date"])

# Restrict to 1992–2015
weather = weather[
    (weather["date"].dt.year >= 1992) & (weather["date"].dt.year <= 2015)
]

# Load station metadata (cities.csv)
cities_path = DATA_DIR / "cities.csv"
cities_df = pd.read_csv(cities_path)

# Merge state info into weather via station_id
# (adjust column names if your file differs)
weather = weather.merge(
    cities_df[["station_id", "state"]].drop_duplicates("station_id"),
    on="station_id",
    how="left",
)

weather = weather.rename(
    columns={
        "state": "STATE",
        "avg_temp_c": "tavg",
        "min_temp_c": "tmin",
        "max_temp_c": "tmax",
        "precipitation_mm": "prcp_1d",  # explicit 1‑day precipitation
        "avg_wind_speed_kmh": "wspd",
        "avg_sea_level_pres_hpa": "pres",
    }
)

weather.head()

# ---- 2.2 Aggregate station data to station–day ----

weather_daily = (
    weather.groupby(["station_id", "date"])
    .agg(
        tavg=("tavg", "mean"),
        tmin=("tmin", "mean"),
        tmax=("tmax", "mean"),
        prcp_1d=("prcp_1d", "sum"),  # daily precip (1‑day)
        wspd=("wspd", "mean"),
        pres=("pres", "mean"),
        STATE=("STATE", "first"),
    )
    .reset_index()
)

weather_daily.head()

# ---- 2.3 Add lagged weather features (per station) ----

def add_lagged_features(
    df,
    group_col,
    date_col,
    base_cols,
    windows=(3, 7, 30),
    shift=1,
):
    """
    For each base_col, compute rolling means or sums over given windows,
    then shift by `shift` days to avoid using same-day info.
    """
    df = df.sort_values([group_col, date_col]).copy()

    for col in base_cols:
        for w in windows:
            roll = (
                df.groupby(group_col)[col]
                .transform(lambda x: x.rolling(w, min_periods=1).mean())
            )
            df[f"{col}_mean_{w}"] = roll.shift(shift)

    # Cumulative precipitation over 7 and 30 days, based on 1‑day precip
    for w in [7, 30]:
        roll = (
            df.groupby(group_col)["prcp_1d"]
            .transform(lambda x: x.rolling(w, min_periods=1).sum())
        )
        df[f"prcp_1d_sum_{w}"] = roll.shift(shift)

    return df


weather_feat = weather_daily.copy()

# Impute any remaining NaNs before building lags (per station)
weather_feat = weather_feat.set_index("station_id")
weather_feat = weather_feat.groupby(level=0).ffill().bfill()
weather_feat = weather_feat.reset_index()

weather_feat["year"] = weather_feat["date"].dt.year
weather_feat["month"] = weather_feat["date"].dt.month
weather_feat["doy"] = weather_feat["date"].dt.dayofyear

base_cols = ["tavg", "tmin", "tmax", "prcp_1d", "wspd", "pres"]

weather_feat = add_lagged_features(
    weather_feat,
    group_col="station_id",
    date_col="date",
    base_cols=base_cols,
)

# Drop first few rows where lags are NaN (from shift)
weather_feat = weather_feat.dropna().reset_index(drop=True)

weather_feat.head()

# ---- 2.3 Add calendar features and lagged weather features ----

def add_lagged_features(
    df,
    group_col,
    date_col,
    base_cols,
    windows=(3, 7, 30),
    shift=1,
):
    """
    For each base_col, compute rolling means or sums (depending on column name)
    over given windows, then shift by `shift` days to avoid using same-day info.
    """
    df = df.sort_values([group_col, date_col]).copy()

    for col in base_cols:
        for w in windows:
            roll = (
                df.groupby(group_col)[col]
                .transform(lambda x: x.rolling(w, min_periods=1).mean())
            )
            df[f"{col}_mean_{w}"] = roll.shift(shift)

    # Example of cumulative precipitation (sums instead of means)
    for w in [7, 30]:
        roll = (
            df.groupby(group_col)["prcp"]
            .transform(lambda x: x.rolling(w, min_periods=1).sum())
        )
        df[f"prcp_sum_{w}"] = roll.shift(shift)

    return df


weather_feat = weather_daily.copy()

# State name to abbreviation mapping to align with fires_daily
state_name_to_abbrev = {
    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',
    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',
    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',
    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',
    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',
    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',
    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',
    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',
    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',
    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',
    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',
    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',
    'Wisconsin': 'WI', 'Wyoming': 'WY',
    'District of Columbia': 'DC'
}

# Apply the mapping to weather_feat['STATE']
weather_feat['STATE'] = weather_feat['STATE'].map(state_name_to_abbrev)

# Drop rows where state mapping resulted in NaN (if any states were not in our map)
weather_feat.dropna(subset=['STATE'], inplace=True)

# Impute any remaining NaNs in weather features before adding lagged features
# This ensures lagged features don't become NaN due to missing base values.
# Explicitly set 'STATE' as index for fill, then reset to column
weather_feat = weather_feat.set_index('STATE')
weather_feat = weather_feat.groupby(level=0).ffill().bfill()
weather_feat = weather_feat.reset_index()

weather_feat["year"] = weather_feat["date"].dt.year
weather_feat["month"] = weather_feat["date"].dt.month
weather_feat["doy"] = weather_feat["date"].dt.dayofyear

base_cols = ["tavg", "tmin", "tmax", "prcp", "wspd", "pres"]
weather_feat = add_lagged_features(
    weather_feat,
    group_col="STATE",
    date_col="date",
    base_cols=base_cols,
)

# Drop first few rows where lags are NaN (expected from the shift operation)
weather_feat = weather_feat.dropna().reset_index(drop=True)

weather_feat.head()

data = weather_feat.merge(
    fires_daily, on=["STATE", "date"], how="left"
)

# Replace NaNs in fire-related columns with 0
fire_cols = [c for c in data.columns if c.startswith("cause_")] + [
    "n_fires",
    "area_burned",
    "any_fire",
]
for c in fire_cols:
    if c in data.columns:
        data[c] = data[c].fillna(0)

# Add transformed target for area
data["log_area_burned"] = np.log1p(data["area_burned"])

# ---- 3.1 Merge weather and fires on station + date ----

data = weather_feat.merge(
    fires_daily, on=["station_id", "date"], how="left"
)

# Replace NaNs in fire-related columns with 0
fire_cols = [c for c in data.columns if c.startswith("cause_")] + [
    "n_fires",
    "area_burned",
    "any_fire",
]
for c in fire_cols:
    if c in data.columns:
        data[c] = data[c].fillna(0)

# Transformed target for area
data["log_area_burned"] = np.log1p(data["area_burned"])

# ---- 3.2 Weekend & holiday features ----

# Weekend indicator
data["is_weekend"] = data["date"].dt.weekday.isin([5, 6]).astype(int)

# US holidays
us_holidays = holidays.US()
data["holiday_name"] = data["date"].dt.date.map(us_holidays.get)
data["is_holiday"] = data["holiday_name"].notna().astype(int)
data["is_july4"] = (data["holiday_name"] == "Independence Day").astype(int)

# Combined "weekend or holiday" flag (optional extra feature)
data["is_weekend_or_holiday"] = (
    (data["is_weekend"] == 1) | (data["is_holiday"] == 1)
).astype(int)

data.head()

# ---- 4.1 Encode STATE and station_id ----

le_state = LabelEncoder()
data["STATE_LE"] = le_state.fit_transform(
    data["STATE"].fillna("UNK").astype(str)
)

le_station = LabelEncoder()
data["station_LE"] = le_station.fit_transform(
    data["station_id"].astype(str)
)

# ---- 4.2 Final feature list (excluding targets) ----
exclude_cols = {
    "n_fires",
    "any_fire",
    "area_burned",
    "log_area_burned",
    "STATE",
    "station_id",
    "date",
} | set([c for c in data.columns if c.startswith("cause_")])

feature_cols = [c for c in data.columns if c not in exclude_cols]
len(feature_cols), feature_cols[:10]

# ---- 5.1 Train / test split by year ----
train_year_cutoff = 2011
train_mask = data["year"] < train_year_cutoff
test_mask = ~train_mask

X_train = data.loc[train_mask, feature_cols]
X_test = data.loc[test_mask, feature_cols]

y_train_count = data.loc[train_mask, "n_fires"]
y_test_count = data.loc[test_mask, "n_fires"]

y_train_bin = data.loc[train_mask, "any_fire"]
y_test_bin = data.loc[test_mask, "any_fire"]

y_train_log_area = data.loc[train_mask, "log_area_burned"]
y_test_log_area = data.loc[test_mask, "log_area_burned"]

# ---- 6.1 Count model (Poisson) ----

lgb_train = lgb.Dataset(X_train, label=y_train_count)
lgb_valid = lgb.Dataset(X_test, label=y_test_count)

params_count = {
    "objective": "poisson",
    "learning_rate": 0.05,
    "num_leaves": 64,
    "metric": "rmse",
    "feature_fraction": 0.8,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "seed": RANDOM_STATE,
}

model_count = lgb.train(
    params_count,
    lgb_train,
    num_boost_round=2000,
    valid_sets=[lgb_valid],
    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],
)

pred_count = model_count.predict(X_test, num_iteration=model_count.best_iteration)
# Fix: Remove squared=False and take np.sqrt to get RMSE
rmse_count = np.sqrt(mean_squared_error(y_test_count, pred_count))

print(f"Count model RMSE: {rmse_count:.3f}")
print(f"Mean actual count in test: {y_test_count.mean():.3f}")

# ---- 6.2 Binary any-fire model (for AUC) ----

params_bin = {
    "objective": "binary",
    "learning_rate": 0.05,
    "num_leaves": 64,
    "metric": "auc",
    "feature_fraction": 0.8,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "seed": RANDOM_STATE,
}

lgb_train_bin = lgb.Dataset(X_train, label=y_train_bin)
lgb_valid_bin = lgb.Dataset(X_test, label=y_test_bin)

model_bin = lgb.train(
    params_bin,
    lgb_train_bin,
    num_boost_round=2000,
    valid_sets=[lgb_valid_bin],
    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],
)

prob_any_fire = model_bin.predict(
    X_test, num_iteration=model_bin.best_iteration
)
auc_any_fire = roc_auc_score(y_test_bin, prob_any_fire)

print(f"AUC (any_fire): {auc_any_fire:.3f}")
print(f"Positive rate (fires on a day): {y_test_bin.mean():.3f}")

# ---- 6.3 Simple clustering-only baseline for counts ----
# Predict the mean count for each weather_cluster on train,
# then apply to test based only on cluster label.

cluster_means = (
    data.loc[train_mask]
    .groupby("weather_cluster")["n_fires"]
    .mean()
)

baseline_pred = data.loc[test_mask, "weather_cluster"].map(cluster_means)
baseline_rmse = np.sqrt(mean_squared_error(
    y_test_count, baseline_pred
))

print(f"Cluster baseline RMSE: {baseline_rmse:.3f}")
print(f"LightGBM Poisson RMSE: {rmse_count:.3f}")

# ---- 7.1 Area model ----

params_area = {
    "objective": "regression",
    "learning_rate": 0.05,
    "num_leaves": 64,
    "metric": "rmse",
    "feature_fraction": 0.8,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "seed": RANDOM_STATE,
}

lgb_train_area = lgb.Dataset(X_train, label=y_train_log_area)
lgb_valid_area = lgb.Dataset(X_test, label=y_test_log_area)

model_area = lgb.train(
    params_area,
    lgb_train_area,
    num_boost_round=2000,
    valid_sets=[lgb_valid_area],
    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=100)],
)

pred_log_area = model_area.predict(
    X_test, num_iteration=model_area.best_iteration
)

rmse_log_area = np.sqrt(mean_squared_error(
    y_test_log_area, pred_log_area
))
r2_log_area = r2_score(y_test_log_area, pred_log_area)

print(f"Area model RMSE (log1p): {rmse_log_area:.3f}")
print(f"Area model R^2 (log1p): {r2_log_area:.3f}")

# Baseline: predict mean log-area of train
baseline_log = np.full_like(y_test_log_area, y_train_log_area.mean())
baseline_rmse_log = np.sqrt(mean_squared_error(
    y_test_log_area, baseline_log
))
print(f"Baseline RMSE (log1p mean): {baseline_rmse_log:.3f}")

# ---- 8.1 Prepare cause targets ----

cause_cols = [c for c in data.columns if c.startswith("cause_")]
len(cause_cols), cause_cols

# Keep only days with at least one fire
data_cause = data[data["n_fires"] > 0].copy()

# Proportions per cause
for c in cause_cols:
    data_cause[c + "_prop"] = data_cause[c] / data_cause["n_fires"]

target_prop_cols = [c + "_prop" for c in cause_cols]

# Train/test split by year (same cutoff)
train_mask_cause = data_cause["year"] < train_year_cutoff
test_mask_cause = ~train_mask_cause

X_train_cause = data_cause.loc[train_mask_cause, feature_cols]
X_test_cause = data_cause.loc[test_mask_cause, feature_cols]

Y_train_cause = data_cause.loc[train_mask_cause, target_prop_cols].values
Y_test_cause = data_cause.loc[test_mask_cause, target_prop_cols].values

# ---- 8.2 Multi-output regression for cause proportions ----

base_reg = LGBMRegressor(
    objective="regression",
    learning_rate=0.05,
    num_leaves=64,
    n_estimators=800,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RANDOM_STATE,
)

multi_reg = MultiOutputRegressor(base_reg)

multi_reg.fit(X_train_cause, Y_train_cause)

Y_pred_cause = multi_reg.predict(X_test_cause)

# Ensure non-negative and normalize to sum to 1
Y_pred_cause = np.clip(Y_pred_cause, 0, None)
row_sums = Y_pred_cause.sum(axis=1, keepdims=True)
row_sums[row_sums == 0] = 1
Y_pred_cause = Y_pred_cause / row_sums

# ---- 8.3 Metrics: MSE of proportions and cross-entropy per fire ----

# Mean squared error on proportions
mse_props = mean_squared_error(Y_test_cause.reshape(-1), Y_pred_cause.reshape(-1))
print(f"MSE on cause proportions: {mse_props:.5f}")

# Cross-entropy (log-loss) per day per fire:
eps = 1e-12
ce_per_day = -np.sum(Y_test_cause * np.log(Y_pred_cause + eps), axis=1)
ce_mean = ce_per_day.mean()
print(f"Mean cross-entropy (per day, per fire): {ce_mean:.3f}")

# Baseline: always predict global average cause distribution (from train)
global_dist = Y_train_cause.sum(axis=0)
global_dist = global_dist / global_dist.sum()

baseline_pred = np.tile(global_dist, (Y_test_cause.shape[0], 1))
ce_baseline = -np.sum(Y_test_cause * np.log(baseline_pred + eps), axis=1).mean()
mse_baseline = mean_squared_error(
    Y_test_cause.reshape(-1), baseline_pred.reshape(-1)
)

print(f"Baseline CE: {ce_baseline:.3f}")
print(f"Baseline MSE: {mse_baseline:.5f}")

# Example: look up a specific state and date
example = data_cause[(data_cause["STATE"] == "CA")].iloc[0]  # first CA day with fires
x_ex = example[feature_cols].values.reshape(1, -1)
p_cause = multi_reg.predict(x_ex)[0]
p_cause = np.clip(p_cause, 0, None)
p_cause = p_cause / p_cause.sum()

prob_by_cause = dict(zip(cause_cols, p_cause))
prob_by_cause

import matplotlib.pyplot as plt
import seaborn as sns

# Predicted vs. Actual for Number of Fires
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test_count, y=pred_count, alpha=0.3)
plt.plot([y_test_count.min(), y_test_count.max()], [y_test_count.min(), y_test_count.max()], 'r--', lw=2)
plt.title('Predicted vs. Actual Number of Fires (Test Set)')
plt.xlabel('Actual Number of Fires')
plt.ylabel('Predicted Number of Fires')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Predicted vs. Actual for Log Area Burned
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test_log_area, y=pred_log_area, alpha=0.3)
plt.plot([y_test_log_area.min(), y_test_log_area.max()], [y_test_log_area.min(), y_test_log_area.max()], 'r--', lw=2)
plt.title('Predicted vs. Actual Log(Area Burned + 1) (Test Set)')
plt.xlabel('Actual Log(Area Burned + 1)')
plt.ylabel('Predicted Log(Area Burned + 1)')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Feature Importance for Area Burned Model
feature_importances_area = pd.DataFrame({
    'feature': X_test.columns,
    'importance': model_area.feature_importance()
}).sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importances_area.head(15))
plt.title('Top 15 Feature Importances for Area Burned Model')
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Feature Importance for specific cause codes (Lightning, Campfire)
# Find the index for 'cause_lightning_prop' (cause code 1) and 'cause_campfire_prop' (cause code 4)
lightning_idx = target_prop_cols.index('cause_1_prop')
campfire_idx = target_prop_cols.index('cause_4_prop')

# Get the individual LightGBM regressors for these causes
model_lightning = multi_reg.estimators_[lightning_idx]
model_campfire = multi_reg.estimators_[campfire_idx]

# Get feature importances for Lightning
feature_importances_lightning = pd.DataFrame({
    'feature': X_test_cause.columns,
    'importance': model_lightning.feature_importances_
}).sort_values('importance', ascending=False)

# Get feature importances for Campfire
feature_importances_campfire = pd.DataFrame({
    'feature': X_test_cause.columns,
    'importance': model_campfire.feature_importances_
}).sort_values('importance', ascending=False)

# Plotting
fig, axes = plt.subplots(1, 2, figsize=(20, 8))

sns.barplot(x='importance', y='feature', data=feature_importances_lightning.head(10), ax=axes[0])
axes[0].set_title('Top 10 Feature Importances for Lightning Cause (Cause 1)')
axes[0].set_xlabel('Feature Importance')
axes[0].set_ylabel('Feature')

sns.barplot(x='importance', y='feature', data=feature_importances_campfire.head(10), ax=axes[1])
axes[1].set_title('Top 10 Feature Importances for Campfire Cause (Cause 4)')
axes[1].set_xlabel('Feature Importance')
axes[1].set_ylabel('Feature')

plt.tight_layout()
plt.show()

# Load only STAT_CAUSE_CODE and STAT_CAUSE_DESCR to create a mapping
# fires_path is already defined in the notebook.
fires_cause_descriptions = pd.read_parquet(
    fires_path,
    columns=["STAT_CAUSE_CODE", "STAT_CAUSE_DESCR"]
)

# Create a unique mapping from code to description
# Handle potential duplicates by taking the first description for each code
cause_code_to_descr = fires_cause_descriptions[['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']].drop_duplicates().set_index('STAT_CAUSE_CODE')['STAT_CAUSE_DESCR'].to_dict()

print("Cause Code to Description Mapping:")
for code, descr in sorted(cause_code_to_descr.items()):
    print(f"{code}: {descr}")

# Prepare the renaming dictionary
rename_dict = {}
for col in data.columns:
    if col.startswith("cause_"):
        cause_code_str = col.split('_')[1] # Extract the numeric code as string
        # Check if the extracted string is actually a digit before converting to int
        if cause_code_str.isdigit():
            cause_code = int(cause_code_str)
            if cause_code in cause_code_to_descr:
                # Create a clean, descriptive column name
                # Replace spaces/special chars with underscores and convert to lowercase
                new_col_name = "cause_" + cause_code_to_descr[cause_code].replace(" ", "_").replace("/", "_").replace("-", "_").replace("(","").replace(")","").lower()
                rename_dict[col] = new_col_name
            else:
                print(f"Warning: No description found for STAT_CAUSE_CODE {cause_code}. Column '{col}' will not be renamed.")
        else:
            # If it's not a digit, it's likely already renamed or not a standard numeric cause column.
            # We can skip renaming it.
            print(f"Info: Column '{col}' is not in 'cause_X' numeric format; skipping renaming.")

# Apply the renaming to the DataFrame
data = data.rename(columns=rename_dict)

# Display the head of the DataFrame to show the new column names
print("DataFrame with renamed cause columns:")
display(data.head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(y_train_log_area, bins=50, kde=True)
plt.title('Distribution of log_area_burned (Training Set) - Recheck')
plt.xlabel('log(Area Burned + 1)')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(y_test_log_area, bins=50, kde=True)
plt.title('Distribution of log_area_burned (Test Set) - Recheck')
plt.xlabel('log(Area Burned + 1)')
plt.ylabel('Frequency')
plt.show()

print('Summary statistics for y_train_log_area (Recheck):')
print(y_train_log_area.describe())

print('\nSummary statistics for y_test_log_area (Recheck):')
print(y_test_log_area.describe())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(y_train_log_area, bins=50, kde=True)
plt.title('Distribution of log_area_burned (Training Set)')
plt.xlabel('log(Area Burned + 1)')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(y_test_log_area, bins=50, kde=True)
plt.title('Distribution of log_area_burned (Test Set)')
plt.xlabel('log(Area Burned + 1)')
plt.ylabel('Frequency')
plt.show()

print('Summary statistics for y_train_log_area:')
print(y_train_log_area.describe())

print('\nSummary statistics for y_test_log_area:')
print(y_test_log_area.describe())